{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LAST_NAME-FIRST_NAME-Assignment-2-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Pgcm7as3nKq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load libraries\n",
        "import numpy as np\n",
        "from keras.datasets import reuters\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras import models\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# Set random seed\n",
        "np.random.seed(44)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt3LVAuCUVOf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9c1281a4-dd46-4543-bf5d-98909ed88dbd"
      },
      "source": [
        "# load data\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
        "# Convert feature data to a one-hot encoded feature matrix\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "train_features = tokenizer.sequences_to_matrix(train_data, mode='binary')\n",
        "test_features = tokenizer.sequences_to_matrix(test_data, mode='binary')\n",
        "\n",
        "# One-hot encode target vector to create a target matrix\n",
        "train_target = to_categorical(train_labels)\n",
        "test_target = to_categorical(test_labels)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz\n",
            "2113536/2110848 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VjE1EcUVZCO",
        "colab_type": "text"
      },
      "source": [
        "# Consider different number of hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcS6wPhTUc00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1 hidden\n",
        "net1 = models.Sequential()\n",
        "\n",
        "# Add fully connected layer with a ReLU activation function\n",
        "net1.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "# Add fully connected layer with a softmax activation function\n",
        "net1.add(layers.Dense(units=46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtFLOKJgUczF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2 hidden\n",
        "net2 = models.Sequential()\n",
        "\n",
        "# Add 2 fully connected layers with a ReLU activation function\n",
        "net2.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net2.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "# Add fully connected layer with a softmax activation function\n",
        "net2.add(layers.Dense(units=46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XvvTiFoUcvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3 hidden\n",
        "net3 = models.Sequential()\n",
        "\n",
        "# Add 3 fully connected layers with a ReLU activation function\n",
        "net3.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net3.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net3.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "# Add fully connected layer with a softmax activation function\n",
        "net3.add(layers.Dense(units=46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slvxHf0EUcuD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4 hidden\n",
        "net4 = models.Sequential()\n",
        "\n",
        "# Add 4 fully connected layers with a ReLU activation function\n",
        "net4.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net4.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net4.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net4.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "# Add fully connected layer with a softmax activation function\n",
        "net4.add(layers.Dense(units=46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRsURKHMUcqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 5 hidden\n",
        "net5 = models.Sequential()\n",
        "\n",
        "# Add 5 fully connected layers with a ReLU activation function\n",
        "net5.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net5.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net5.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net5.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net5.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "\n",
        "# Add fully connected layer with a softmax activation function\n",
        "net5.add(layers.Dense(units=46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RLt-VF4UcnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net1.compile(loss='categorical_crossentropy', # Cross-entropy\n",
        "                optimizer='rmsprop', # Root Mean Square Propagation\n",
        "                metrics=['accuracy']) # Accuracy performance metric\n",
        "\n",
        "net2.compile(loss='categorical_crossentropy', # Cross-entropy\n",
        "                optimizer='rmsprop', # Root Mean Square Propagation\n",
        "                metrics=['accuracy']) # Accuracy performance metric\n",
        "        \n",
        "net3.compile(loss='categorical_crossentropy', # Cross-entropy\n",
        "                optimizer='rmsprop', # Root Mean Square Propagation\n",
        "                metrics=['accuracy']) # Accuracy performance metric\n",
        "\n",
        "net4.compile(loss='categorical_crossentropy', # Cross-entropy\n",
        "                optimizer='rmsprop', # Root Mean Square Propagation\n",
        "                metrics=['accuracy']) # Accuracy performance metric\n",
        "\n",
        "net5.compile(loss='categorical_crossentropy', # Cross-entropy\n",
        "                optimizer='rmsprop', # Root Mean Square Propagation\n",
        "                metrics=['accuracy']) # Accuracy performance metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHX0xITZUcmF",
        "colab_type": "code",
        "outputId": "dd415df7-0e1d-48e8-a958-c12c4ea8ce39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Train neural network\n",
        "history1 = net1.fit(train_features, # Features\n",
        "                      train_target, # Target vector\n",
        "                      epochs=3, # Three epochs\n",
        "                      verbose=1, # short output\n",
        "                      batch_size=100, # Number of observations per batch\n",
        "                      validation_data=(test_features, test_target)) # Data to use for evaluation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 2s 253us/step - loss: 0.1787 - accuracy: 0.9511 - val_loss: 1.0154 - val_accuracy: 0.8032\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 2s 257us/step - loss: 0.1616 - accuracy: 0.9522 - val_loss: 1.0383 - val_accuracy: 0.8045\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 2s 256us/step - loss: 0.1510 - accuracy: 0.9541 - val_loss: 1.1364 - val_accuracy: 0.7983\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm9EtFiSUckF",
        "colab_type": "code",
        "outputId": "4d9ae9f4-96cc-4c28-a842-bb65eca6af6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "history2 = net2.fit(train_features,\n",
        "                      train_target,\n",
        "                      epochs=3, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 2s 253us/step - loss: 1.5012 - accuracy: 0.6848 - val_loss: 1.0875 - val_accuracy: 0.7524\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 2s 247us/step - loss: 0.7415 - accuracy: 0.8427 - val_loss: 0.9289 - val_accuracy: 0.7885\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 2s 247us/step - loss: 0.4466 - accuracy: 0.9041 - val_loss: 0.9050 - val_accuracy: 0.8050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9iWhRivUcgG",
        "colab_type": "code",
        "outputId": "0f774ca0-1378-456f-b2e5-7e123ef8f6e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "history3 = net3.fit(train_features,\n",
        "                      train_target,\n",
        "                      epochs=3, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 2s 261us/step - loss: 1.5454 - accuracy: 0.6572 - val_loss: 1.2104 - val_accuracy: 0.7182\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 2s 251us/step - loss: 0.8055 - accuracy: 0.8183 - val_loss: 0.9571 - val_accuracy: 0.7867\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 2s 248us/step - loss: 0.4858 - accuracy: 0.8912 - val_loss: 0.9419 - val_accuracy: 0.8014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdwziZbtUcdG",
        "colab_type": "code",
        "outputId": "cda921c6-4bd8-4811-8e67-0c3e64a66c96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "history4 = net4.fit(train_features,\n",
        "                      train_target,\n",
        "                      epochs=3, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 2s 260us/step - loss: 1.5702 - accuracy: 0.6477 - val_loss: 1.1643 - val_accuracy: 0.7337\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 2s 251us/step - loss: 0.8312 - accuracy: 0.8059 - val_loss: 0.9960 - val_accuracy: 0.7756\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 2s 254us/step - loss: 0.5104 - accuracy: 0.8781 - val_loss: 1.0412 - val_accuracy: 0.7792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8xUPY8QUcaY",
        "colab_type": "code",
        "outputId": "0c9b8467-2c37-43e7-96af-0b887132f1fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "history5 = net5.fit(train_features,\n",
        "                      train_target,\n",
        "                      epochs=3, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 2s 270us/step - loss: 1.6221 - accuracy: 0.6193 - val_loss: 1.2378 - val_accuracy: 0.7110\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 2s 256us/step - loss: 0.9270 - accuracy: 0.7791 - val_loss: 1.1017 - val_accuracy: 0.7600\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 2s 258us/step - loss: 0.5994 - accuracy: 0.8595 - val_loss: 1.0983 - val_accuracy: 0.7743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nWLTVdcXtZk",
        "colab_type": "text"
      },
      "source": [
        "## shallow net with 2 hidden layers performs the best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uWHR77QYIZm",
        "colab_type": "text"
      },
      "source": [
        "# Consider different number of units per hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdLHNsgeUcXV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net100 = models.Sequential()\n",
        "net100.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net100.add(layers.Dense(units=100, activation='relu', input_shape=(10000,)))\n",
        "net100.add(layers.Dense(units=46, activation='softmax'))\n",
        "\n",
        "net200 = models.Sequential()\n",
        "net200.add(layers.Dense(units=200, activation='relu', input_shape=(10000,)))\n",
        "net200.add(layers.Dense(units=200, activation='relu', input_shape=(10000,)))\n",
        "net200.add(layers.Dense(units=46, activation='softmax'))\n",
        "\n",
        "net300 = models.Sequential()\n",
        "net300.add(layers.Dense(units=300, activation='relu', input_shape=(10000,)))\n",
        "net300.add(layers.Dense(units=300, activation='relu', input_shape=(10000,)))\n",
        "net300.add(layers.Dense(units=46, activation='softmax'))\n",
        "\n",
        "net400 = models.Sequential()\n",
        "net400.add(layers.Dense(units=400, activation='relu', input_shape=(10000,)))\n",
        "net400.add(layers.Dense(units=400, activation='relu', input_shape=(10000,)))\n",
        "net400.add(layers.Dense(units=46, activation='softmax'))\n",
        "\n",
        "net500 = models.Sequential()\n",
        "net500.add(layers.Dense(units=500, activation='relu', input_shape=(10000,)))\n",
        "net500.add(layers.Dense(units=500, activation='relu', input_shape=(10000,)))\n",
        "net500.add(layers.Dense(units=46, activation='softmax'))\n",
        "\n",
        "net100200 = models.Sequential()\n",
        "net100200.add(layers.Dense(units=200, activation='relu', input_shape=(10000,)))\n",
        "net100200.add(layers.Dense(units=200, activation='relu', input_shape=(10000,)))\n",
        "net100200.add(layers.Dense(units=46, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwXqbB7OX6zx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net100.compile(loss='categorical_crossentropy', # Cross-entropy\n",
        "                optimizer='rmsprop', # Root Mean Square Propagation\n",
        "                metrics=['accuracy']) # Accuracy performance metric\n",
        "\n",
        "net200.compile(loss='categorical_crossentropy', # Cross-entropy\n",
        "                optimizer='rmsprop', # Root Mean Square Propagation\n",
        "                metrics=['accuracy']) # Accuracy performance metric\n",
        "        \n",
        "net300.compile(loss='categorical_crossentropy', # Cross-entropy\n",
        "                optimizer='rmsprop', # Root Mean Square Propagation\n",
        "                metrics=['accuracy']) # Accuracy performance metric\n",
        "\n",
        "net400.compile(loss='categorical_crossentropy', # Cross-entropy\n",
        "                optimizer='rmsprop', # Root Mean Square Propagation\n",
        "                metrics=['accuracy']) # Accuracy performance metric\n",
        "\n",
        "net500.compile(loss='categorical_crossentropy', # Cross-entropy\n",
        "                optimizer='rmsprop', # Root Mean Square Propagation\n",
        "                metrics=['accuracy']) # Accuracy performance metric\n",
        "      \n",
        "net100200.compile(loss='categorical_crossentropy', # Cross-entropy\n",
        "                optimizer='rmsprop', # Root Mean Square Propagation\n",
        "                metrics=['accuracy']) # Accuracy performance metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeB-s9p_X6wI",
        "colab_type": "code",
        "outputId": "26ed40b4-954a-4fb1-814a-63121cdf703c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "history100 = net100.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=3, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 2s 251us/step - loss: 0.3062 - accuracy: 0.9320 - val_loss: 0.9144 - val_accuracy: 0.8041\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 2s 246us/step - loss: 0.2323 - accuracy: 0.9441 - val_loss: 1.0108 - val_accuracy: 0.8019\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 2s 249us/step - loss: 0.2033 - accuracy: 0.9463 - val_loss: 1.0270 - val_accuracy: 0.8010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j36f689X6tv",
        "colab_type": "code",
        "outputId": "0b7bc6f7-feca-47f8-e56f-83c0c21324a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "history200 = net200.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=3, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 4s 411us/step - loss: 1.3747 - accuracy: 0.6978 - val_loss: 1.0250 - val_accuracy: 0.7654\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 4s 406us/step - loss: 0.6078 - accuracy: 0.8705 - val_loss: 0.8974 - val_accuracy: 0.8032\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 4s 402us/step - loss: 0.3438 - accuracy: 0.9257 - val_loss: 0.8991 - val_accuracy: 0.8077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d2ROoN3X6rn",
        "colab_type": "code",
        "outputId": "971d8209-326a-4a4b-c801-06e9d4a5cf72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "history300 = net300.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=3, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 5s 593us/step - loss: 1.2876 - accuracy: 0.7144 - val_loss: 0.9499 - val_accuracy: 0.7805\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 5s 583us/step - loss: 0.5451 - accuracy: 0.8778 - val_loss: 0.8826 - val_accuracy: 0.8014\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 5s 579us/step - loss: 0.3050 - accuracy: 0.9311 - val_loss: 0.9368 - val_accuracy: 0.8094\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdyVhEFcX6pJ",
        "colab_type": "code",
        "outputId": "fa877051-cf15-4cff-8461-3305a15f9f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "history400 = net400.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=3, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 7s 742us/step - loss: 1.2853 - accuracy: 0.7122 - val_loss: 0.9434 - val_accuracy: 0.7792\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 7s 729us/step - loss: 0.5151 - accuracy: 0.8833 - val_loss: 0.9124 - val_accuracy: 0.7965\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 7s 725us/step - loss: 0.2940 - accuracy: 0.9343 - val_loss: 0.9898 - val_accuracy: 0.7961\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObdVnvM4X6m7",
        "colab_type": "code",
        "outputId": "85273b67-a7d3-488d-90ae-2a20219b4099",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "history500 = net500.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=3, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 8s 918us/step - loss: 1.2581 - accuracy: 0.7149 - val_loss: 0.9069 - val_accuracy: 0.7898\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 8s 906us/step - loss: 0.5025 - accuracy: 0.8842 - val_loss: 0.8781 - val_accuracy: 0.8063\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 8s 906us/step - loss: 0.2767 - accuracy: 0.9353 - val_loss: 0.9739 - val_accuracy: 0.8063\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH70P8uyX6jk",
        "colab_type": "code",
        "outputId": "65bd5791-c40a-4afb-8bf7-7232ebebb3c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "history100200 = net100200.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=3, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target)) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 4s 406us/step - loss: 1.3409 - accuracy: 0.7049 - val_loss: 1.0404 - val_accuracy: 0.7649\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 4s 395us/step - loss: 0.6057 - accuracy: 0.8668 - val_loss: 0.9734 - val_accuracy: 0.7765\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 4s 395us/step - loss: 0.3403 - accuracy: 0.9251 - val_loss: 0.9112 - val_accuracy: 0.8010\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cga8y2YzZwSv",
        "colab_type": "text"
      },
      "source": [
        "## shallow net with 2 hidden layers and 300 units per layer performs the best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqRWk8amZ98e",
        "colab_type": "text"
      },
      "source": [
        "# Consider different number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP0gD96DX6iP",
        "colab_type": "code",
        "outputId": "6917529a-9be1-4e66-caad-dc7accf12b83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for ep in [3, 10, 20, 30, 40]:\n",
        "    print(f'num of epochs: {ep}')\n",
        "    print('_'*50)\n",
        "    history = net300.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=ep, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target))\n",
        "    print('Accuracy: ', np.array(history.history['accuracy']).mean())\n",
        "    print('Accuracy val: ', np.array(history.history['val_accuracy']).mean())\n",
        "    print('_'*50)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num of epochs: 3\n",
            "__________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 5s 562us/step - loss: 0.2220 - accuracy: 0.9450 - val_loss: 0.9970 - val_accuracy: 0.8041\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 5s 555us/step - loss: 0.1839 - accuracy: 0.9492 - val_loss: 1.1364 - val_accuracy: 0.8014\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 5s 567us/step - loss: 0.1614 - accuracy: 0.9516 - val_loss: 1.2242 - val_accuracy: 0.7983\n",
            "Accuracy:  0.94860095\n",
            "Accuracy val:  0.8012763261795044\n",
            "__________________________________________________\n",
            "num of epochs: 10\n",
            "__________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/10\n",
            "8982/8982 [==============================] - 5s 564us/step - loss: 0.1416 - accuracy: 0.9523 - val_loss: 1.3695 - val_accuracy: 0.7992\n",
            "Epoch 2/10\n",
            "8982/8982 [==============================] - 5s 569us/step - loss: 0.1296 - accuracy: 0.9525 - val_loss: 1.4365 - val_accuracy: 0.7894\n",
            "Epoch 3/10\n",
            "8982/8982 [==============================] - 5s 592us/step - loss: 0.1195 - accuracy: 0.9523 - val_loss: 1.6060 - val_accuracy: 0.7988\n",
            "Epoch 4/10\n",
            "8982/8982 [==============================] - 5s 591us/step - loss: 0.1085 - accuracy: 0.9535 - val_loss: 1.8401 - val_accuracy: 0.7832\n",
            "Epoch 5/10\n",
            "8982/8982 [==============================] - 5s 582us/step - loss: 0.1018 - accuracy: 0.9534 - val_loss: 1.8999 - val_accuracy: 0.7903\n",
            "Epoch 6/10\n",
            "8982/8982 [==============================] - 5s 591us/step - loss: 0.0978 - accuracy: 0.9544 - val_loss: 2.0141 - val_accuracy: 0.7921\n",
            "Epoch 7/10\n",
            "8982/8982 [==============================] - 5s 585us/step - loss: 0.0896 - accuracy: 0.9554 - val_loss: 2.2468 - val_accuracy: 0.7854\n",
            "Epoch 8/10\n",
            "8982/8982 [==============================] - 5s 576us/step - loss: 0.0881 - accuracy: 0.9544 - val_loss: 2.3622 - val_accuracy: 0.7921\n",
            "Epoch 9/10\n",
            "8982/8982 [==============================] - 5s 566us/step - loss: 0.0830 - accuracy: 0.9546 - val_loss: 2.5665 - val_accuracy: 0.7783\n",
            "Epoch 10/10\n",
            "8982/8982 [==============================] - 5s 556us/step - loss: 0.0798 - accuracy: 0.9545 - val_loss: 2.7027 - val_accuracy: 0.7898\n",
            "Accuracy:  0.9537074\n",
            "Accuracy val:  0.7898486256599426\n",
            "__________________________________________________\n",
            "num of epochs: 20\n",
            "__________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/20\n",
            "8982/8982 [==============================] - 5s 559us/step - loss: 0.0804 - accuracy: 0.9565 - val_loss: 2.8239 - val_accuracy: 0.7854\n",
            "Epoch 2/20\n",
            "8982/8982 [==============================] - 5s 564us/step - loss: 0.0785 - accuracy: 0.9562 - val_loss: 3.0053 - val_accuracy: 0.7854\n",
            "Epoch 3/20\n",
            "8982/8982 [==============================] - 5s 568us/step - loss: 0.0742 - accuracy: 0.9551 - val_loss: 3.2255 - val_accuracy: 0.7858\n",
            "Epoch 4/20\n",
            "8982/8982 [==============================] - 5s 582us/step - loss: 0.0785 - accuracy: 0.9559 - val_loss: 3.5910 - val_accuracy: 0.7885\n",
            "Epoch 5/20\n",
            "8982/8982 [==============================] - 5s 599us/step - loss: 0.0759 - accuracy: 0.9565 - val_loss: 3.2581 - val_accuracy: 0.7854\n",
            "Epoch 6/20\n",
            "8982/8982 [==============================] - 5s 593us/step - loss: 0.0702 - accuracy: 0.9540 - val_loss: 3.4917 - val_accuracy: 0.7858\n",
            "Epoch 7/20\n",
            "8982/8982 [==============================] - 5s 592us/step - loss: 0.0730 - accuracy: 0.9570 - val_loss: 3.7620 - val_accuracy: 0.7796\n",
            "Epoch 8/20\n",
            "8982/8982 [==============================] - 5s 597us/step - loss: 0.0707 - accuracy: 0.9572 - val_loss: 3.4728 - val_accuracy: 0.7858\n",
            "Epoch 9/20\n",
            "8982/8982 [==============================] - 5s 584us/step - loss: 0.0733 - accuracy: 0.9559 - val_loss: 3.9927 - val_accuracy: 0.7747\n",
            "Epoch 10/20\n",
            "8982/8982 [==============================] - 5s 591us/step - loss: 0.0681 - accuracy: 0.9590 - val_loss: 4.0063 - val_accuracy: 0.7805\n",
            "Epoch 11/20\n",
            "8982/8982 [==============================] - 5s 593us/step - loss: 0.0708 - accuracy: 0.9571 - val_loss: 4.5611 - val_accuracy: 0.7836\n",
            "Epoch 12/20\n",
            "8982/8982 [==============================] - 5s 591us/step - loss: 0.0738 - accuracy: 0.9554 - val_loss: 4.2508 - val_accuracy: 0.7845\n",
            "Epoch 13/20\n",
            "8982/8982 [==============================] - 5s 586us/step - loss: 0.0710 - accuracy: 0.9576 - val_loss: 4.3507 - val_accuracy: 0.7818\n",
            "Epoch 14/20\n",
            "8982/8982 [==============================] - 5s 568us/step - loss: 0.0691 - accuracy: 0.9566 - val_loss: 4.3641 - val_accuracy: 0.7818\n",
            "Epoch 15/20\n",
            "8982/8982 [==============================] - 5s 557us/step - loss: 0.0732 - accuracy: 0.9566 - val_loss: 4.6772 - val_accuracy: 0.7832\n",
            "Epoch 16/20\n",
            "8982/8982 [==============================] - 5s 562us/step - loss: 0.0708 - accuracy: 0.9571 - val_loss: 4.6848 - val_accuracy: 0.7809\n",
            "Epoch 17/20\n",
            "8982/8982 [==============================] - 5s 553us/step - loss: 0.0672 - accuracy: 0.9587 - val_loss: 4.6613 - val_accuracy: 0.7814\n",
            "Epoch 18/20\n",
            "8982/8982 [==============================] - 5s 560us/step - loss: 0.0681 - accuracy: 0.9562 - val_loss: 4.5821 - val_accuracy: 0.7832\n",
            "Epoch 19/20\n",
            "8982/8982 [==============================] - 5s 556us/step - loss: 0.0692 - accuracy: 0.9584 - val_loss: 5.1395 - val_accuracy: 0.7814\n",
            "Epoch 20/20\n",
            "8982/8982 [==============================] - 5s 557us/step - loss: 0.0680 - accuracy: 0.9584 - val_loss: 5.0594 - val_accuracy: 0.7814\n",
            "Accuracy:  0.95677453\n",
            "Accuracy val:  0.7830142498016357\n",
            "__________________________________________________\n",
            "num of epochs: 30\n",
            "__________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/30\n",
            "8982/8982 [==============================] - 5s 557us/step - loss: 0.0683 - accuracy: 0.9587 - val_loss: 5.4041 - val_accuracy: 0.7850\n",
            "Epoch 2/30\n",
            "8982/8982 [==============================] - 5s 559us/step - loss: 0.0710 - accuracy: 0.9572 - val_loss: 5.0797 - val_accuracy: 0.7760\n",
            "Epoch 3/30\n",
            "8982/8982 [==============================] - 5s 564us/step - loss: 0.0655 - accuracy: 0.9589 - val_loss: 5.4834 - val_accuracy: 0.7756\n",
            "Epoch 4/30\n",
            "8982/8982 [==============================] - 5s 558us/step - loss: 0.0688 - accuracy: 0.9589 - val_loss: 5.4846 - val_accuracy: 0.7716\n",
            "Epoch 5/30\n",
            "8982/8982 [==============================] - 5s 559us/step - loss: 0.0644 - accuracy: 0.9589 - val_loss: 5.7489 - val_accuracy: 0.7769\n",
            "Epoch 6/30\n",
            "8982/8982 [==============================] - 5s 562us/step - loss: 0.0688 - accuracy: 0.9585 - val_loss: 6.0584 - val_accuracy: 0.7671\n",
            "Epoch 7/30\n",
            "8982/8982 [==============================] - 5s 570us/step - loss: 0.0763 - accuracy: 0.9585 - val_loss: 5.7438 - val_accuracy: 0.7720\n",
            "Epoch 8/30\n",
            "8982/8982 [==============================] - 5s 591us/step - loss: 0.0634 - accuracy: 0.9587 - val_loss: 6.7446 - val_accuracy: 0.7685\n",
            "Epoch 9/30\n",
            "8982/8982 [==============================] - 5s 585us/step - loss: 0.0667 - accuracy: 0.9579 - val_loss: 6.0408 - val_accuracy: 0.7747\n",
            "Epoch 10/30\n",
            "8982/8982 [==============================] - 5s 593us/step - loss: 0.0719 - accuracy: 0.9590 - val_loss: 5.8589 - val_accuracy: 0.7680\n",
            "Epoch 11/30\n",
            "8982/8982 [==============================] - 5s 594us/step - loss: 0.0627 - accuracy: 0.9590 - val_loss: 6.8058 - val_accuracy: 0.7707\n",
            "Epoch 12/30\n",
            "8982/8982 [==============================] - 5s 590us/step - loss: 0.0660 - accuracy: 0.9590 - val_loss: 6.5612 - val_accuracy: 0.7720\n",
            "Epoch 13/30\n",
            "8982/8982 [==============================] - 5s 573us/step - loss: 0.0656 - accuracy: 0.9591 - val_loss: 6.8539 - val_accuracy: 0.7689\n",
            "Epoch 14/30\n",
            "8982/8982 [==============================] - 5s 562us/step - loss: 0.0667 - accuracy: 0.9595 - val_loss: 7.1088 - val_accuracy: 0.7663\n",
            "Epoch 15/30\n",
            "8982/8982 [==============================] - 5s 581us/step - loss: 0.0706 - accuracy: 0.9580 - val_loss: 7.6654 - val_accuracy: 0.7711\n",
            "Epoch 16/30\n",
            "8982/8982 [==============================] - 5s 574us/step - loss: 0.0682 - accuracy: 0.9595 - val_loss: 7.1610 - val_accuracy: 0.7667\n",
            "Epoch 17/30\n",
            "8982/8982 [==============================] - 5s 557us/step - loss: 0.0638 - accuracy: 0.9578 - val_loss: 8.5942 - val_accuracy: 0.7658\n",
            "Epoch 18/30\n",
            "8982/8982 [==============================] - 5s 564us/step - loss: 0.0787 - accuracy: 0.9577 - val_loss: 7.7537 - val_accuracy: 0.7685\n",
            "Epoch 19/30\n",
            "8982/8982 [==============================] - 5s 560us/step - loss: 0.0632 - accuracy: 0.9587 - val_loss: 8.3883 - val_accuracy: 0.7658\n",
            "Epoch 20/30\n",
            "8982/8982 [==============================] - 5s 574us/step - loss: 0.0653 - accuracy: 0.9595 - val_loss: 8.3777 - val_accuracy: 0.7631\n",
            "Epoch 21/30\n",
            "8982/8982 [==============================] - 5s 566us/step - loss: 0.0694 - accuracy: 0.9587 - val_loss: 7.6497 - val_accuracy: 0.7689\n",
            "Epoch 22/30\n",
            "8982/8982 [==============================] - 5s 568us/step - loss: 0.0671 - accuracy: 0.9600 - val_loss: 7.5853 - val_accuracy: 0.7622\n",
            "Epoch 23/30\n",
            "8982/8982 [==============================] - 5s 569us/step - loss: 0.0683 - accuracy: 0.9580 - val_loss: 8.1374 - val_accuracy: 0.7654\n",
            "Epoch 24/30\n",
            "8982/8982 [==============================] - 5s 582us/step - loss: 0.0685 - accuracy: 0.9589 - val_loss: 8.1347 - val_accuracy: 0.7640\n",
            "Epoch 25/30\n",
            "8982/8982 [==============================] - 5s 604us/step - loss: 0.0899 - accuracy: 0.9608 - val_loss: 8.1645 - val_accuracy: 0.7573\n",
            "Epoch 26/30\n",
            "8982/8982 [==============================] - 5s 596us/step - loss: 0.0627 - accuracy: 0.9609 - val_loss: 9.2538 - val_accuracy: 0.7569\n",
            "Epoch 27/30\n",
            "8982/8982 [==============================] - 5s 597us/step - loss: 0.0700 - accuracy: 0.9596 - val_loss: 8.5297 - val_accuracy: 0.7551\n",
            "Epoch 28/30\n",
            "8982/8982 [==============================] - 5s 593us/step - loss: 0.0731 - accuracy: 0.9594 - val_loss: 9.1640 - val_accuracy: 0.7529\n",
            "Epoch 29/30\n",
            "8982/8982 [==============================] - 5s 597us/step - loss: 0.0647 - accuracy: 0.9586 - val_loss: 9.3347 - val_accuracy: 0.7582\n",
            "Epoch 30/30\n",
            "8982/8982 [==============================] - 5s 584us/step - loss: 0.0717 - accuracy: 0.9594 - val_loss: 9.3324 - val_accuracy: 0.7471\n",
            "Accuracy:  0.95891416\n",
            "Accuracy val:  0.7667557140191396\n",
            "__________________________________________________\n",
            "num of epochs: 40\n",
            "__________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/40\n",
            "8982/8982 [==============================] - 5s 588us/step - loss: 0.0849 - accuracy: 0.9593 - val_loss: 9.2933 - val_accuracy: 0.7631\n",
            "Epoch 2/40\n",
            "8982/8982 [==============================] - 5s 588us/step - loss: 0.0798 - accuracy: 0.9599 - val_loss: 9.6550 - val_accuracy: 0.7502\n",
            "Epoch 3/40\n",
            "8982/8982 [==============================] - 5s 598us/step - loss: 0.0658 - accuracy: 0.9588 - val_loss: 10.1013 - val_accuracy: 0.7529\n",
            "Epoch 4/40\n",
            "8982/8982 [==============================] - 5s 585us/step - loss: 0.0723 - accuracy: 0.9579 - val_loss: 10.5021 - val_accuracy: 0.7520\n",
            "Epoch 5/40\n",
            "8982/8982 [==============================] - 5s 576us/step - loss: 0.0737 - accuracy: 0.9581 - val_loss: 10.5877 - val_accuracy: 0.7520\n",
            "Epoch 6/40\n",
            "8982/8982 [==============================] - 5s 581us/step - loss: 0.0628 - accuracy: 0.9613 - val_loss: 10.1496 - val_accuracy: 0.7507\n",
            "Epoch 7/40\n",
            "8982/8982 [==============================] - 5s 598us/step - loss: 0.0705 - accuracy: 0.9598 - val_loss: 10.1073 - val_accuracy: 0.7409\n",
            "Epoch 8/40\n",
            "8982/8982 [==============================] - 5s 587us/step - loss: 0.0643 - accuracy: 0.9587 - val_loss: 11.2013 - val_accuracy: 0.7551\n",
            "Epoch 9/40\n",
            "8982/8982 [==============================] - 5s 595us/step - loss: 0.0844 - accuracy: 0.9599 - val_loss: 10.8476 - val_accuracy: 0.7409\n",
            "Epoch 10/40\n",
            "8982/8982 [==============================] - 5s 581us/step - loss: 0.0676 - accuracy: 0.9596 - val_loss: 11.6946 - val_accuracy: 0.7467\n",
            "Epoch 11/40\n",
            "8982/8982 [==============================] - 5s 592us/step - loss: 0.0837 - accuracy: 0.9611 - val_loss: 10.5549 - val_accuracy: 0.7489\n",
            "Epoch 12/40\n",
            "8982/8982 [==============================] - 5s 564us/step - loss: 0.0641 - accuracy: 0.9597 - val_loss: 11.0454 - val_accuracy: 0.7471\n",
            "Epoch 13/40\n",
            "8982/8982 [==============================] - 5s 566us/step - loss: 0.0689 - accuracy: 0.9600 - val_loss: 12.1582 - val_accuracy: 0.7337\n",
            "Epoch 14/40\n",
            "8982/8982 [==============================] - 5s 562us/step - loss: 0.0796 - accuracy: 0.9604 - val_loss: 11.4861 - val_accuracy: 0.7378\n",
            "Epoch 15/40\n",
            "8982/8982 [==============================] - 5s 569us/step - loss: 0.0670 - accuracy: 0.9606 - val_loss: 11.3581 - val_accuracy: 0.7458\n",
            "Epoch 16/40\n",
            "8982/8982 [==============================] - 5s 560us/step - loss: 0.0681 - accuracy: 0.9601 - val_loss: 12.5798 - val_accuracy: 0.7324\n",
            "Epoch 17/40\n",
            "8982/8982 [==============================] - 5s 565us/step - loss: 0.0747 - accuracy: 0.9599 - val_loss: 13.0619 - val_accuracy: 0.7391\n",
            "Epoch 18/40\n",
            "8982/8982 [==============================] - 5s 560us/step - loss: 0.0971 - accuracy: 0.9595 - val_loss: 11.8791 - val_accuracy: 0.7311\n",
            "Epoch 19/40\n",
            "8982/8982 [==============================] - 5s 565us/step - loss: 0.0679 - accuracy: 0.9584 - val_loss: 13.3654 - val_accuracy: 0.7493\n",
            "Epoch 20/40\n",
            "8982/8982 [==============================] - 5s 557us/step - loss: 0.0689 - accuracy: 0.9572 - val_loss: 12.7617 - val_accuracy: 0.7373\n",
            "Epoch 21/40\n",
            "8982/8982 [==============================] - 5s 564us/step - loss: 0.0663 - accuracy: 0.9594 - val_loss: 13.6540 - val_accuracy: 0.7498\n",
            "Epoch 22/40\n",
            "8982/8982 [==============================] - 5s 556us/step - loss: 0.1029 - accuracy: 0.9606 - val_loss: 12.7991 - val_accuracy: 0.7329\n",
            "Epoch 23/40\n",
            "8982/8982 [==============================] - 5s 563us/step - loss: 0.0670 - accuracy: 0.9591 - val_loss: 13.7823 - val_accuracy: 0.7444\n",
            "Epoch 24/40\n",
            "8982/8982 [==============================] - 5s 565us/step - loss: 0.0754 - accuracy: 0.9609 - val_loss: 13.0703 - val_accuracy: 0.7391\n",
            "Epoch 25/40\n",
            "8982/8982 [==============================] - 5s 568us/step - loss: 0.0670 - accuracy: 0.9608 - val_loss: 14.0125 - val_accuracy: 0.7373\n",
            "Epoch 26/40\n",
            "8982/8982 [==============================] - 5s 564us/step - loss: 0.0778 - accuracy: 0.9595 - val_loss: 14.4966 - val_accuracy: 0.7262\n",
            "Epoch 27/40\n",
            "8982/8982 [==============================] - 5s 563us/step - loss: 0.0672 - accuracy: 0.9604 - val_loss: 14.3948 - val_accuracy: 0.7177\n",
            "Epoch 28/40\n",
            "8982/8982 [==============================] - 5s 564us/step - loss: 0.0860 - accuracy: 0.9604 - val_loss: 13.5927 - val_accuracy: 0.7333\n",
            "Epoch 29/40\n",
            "8982/8982 [==============================] - 5s 566us/step - loss: 0.0672 - accuracy: 0.9616 - val_loss: 14.8170 - val_accuracy: 0.7262\n",
            "Epoch 30/40\n",
            "8982/8982 [==============================] - 5s 564us/step - loss: 0.1012 - accuracy: 0.9600 - val_loss: 16.3023 - val_accuracy: 0.7235\n",
            "Epoch 31/40\n",
            "8982/8982 [==============================] - 5s 565us/step - loss: 0.0680 - accuracy: 0.9603 - val_loss: 15.3896 - val_accuracy: 0.7222\n",
            "Epoch 32/40\n",
            "8982/8982 [==============================] - 5s 554us/step - loss: 0.0655 - accuracy: 0.9611 - val_loss: 16.4024 - val_accuracy: 0.7271\n",
            "Epoch 33/40\n",
            "8982/8982 [==============================] - 5s 557us/step - loss: 0.0799 - accuracy: 0.9591 - val_loss: 16.1264 - val_accuracy: 0.7168\n",
            "Epoch 34/40\n",
            "8982/8982 [==============================] - 5s 558us/step - loss: 0.0759 - accuracy: 0.9597 - val_loss: 15.5469 - val_accuracy: 0.7146\n",
            "Epoch 35/40\n",
            "8982/8982 [==============================] - 5s 554us/step - loss: 0.0852 - accuracy: 0.9599 - val_loss: 15.9880 - val_accuracy: 0.7226\n",
            "Epoch 36/40\n",
            "8982/8982 [==============================] - 5s 557us/step - loss: 0.0666 - accuracy: 0.9609 - val_loss: 16.5952 - val_accuracy: 0.7133\n",
            "Epoch 37/40\n",
            "8982/8982 [==============================] - 5s 558us/step - loss: 0.0671 - accuracy: 0.9618 - val_loss: 16.9363 - val_accuracy: 0.7213\n",
            "Epoch 38/40\n",
            "8982/8982 [==============================] - 5s 559us/step - loss: 0.0730 - accuracy: 0.9606 - val_loss: 16.7118 - val_accuracy: 0.7168\n",
            "Epoch 39/40\n",
            "8982/8982 [==============================] - 5s 555us/step - loss: 0.1202 - accuracy: 0.9590 - val_loss: 16.1974 - val_accuracy: 0.7044\n",
            "Epoch 40/40\n",
            "8982/8982 [==============================] - 5s 553us/step - loss: 0.0730 - accuracy: 0.9603 - val_loss: 16.0921 - val_accuracy: 0.6906\n",
            "Accuracy:  0.9598921\n",
            "Accuracy val:  0.7346727505326272\n",
            "__________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh5_fPcFdrCZ",
        "colab_type": "text"
      },
      "source": [
        "# Finally we get the best model: 2 hidden layers, 300 units per layer, 3 epochs to train. the best accuracy 0.81"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XBl-wtjelgR",
        "colab_type": "text"
      },
      "source": [
        "# Consider different batch sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oaC09K-vX6ee",
        "colab_type": "code",
        "outputId": "1f353858-6fec-4ac1-ffff-f7fa2537bc30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "loss = []\n",
        "loss_val = []\n",
        "for bsize in [16, 32, 64, 128, 256, 512]:\n",
        "    print(f'batch size: {bsize}')\n",
        "    print('_'*50)\n",
        "    history = net300.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=3, \n",
        "                      verbose=1,\n",
        "                      batch_size=bsize, \n",
        "                      validation_data=(test_features, test_target))\n",
        "    loss.append(np.array(history.history['loss']).mean())\n",
        "    loss_val.append(np.array(history.history['val_loss']).mean())\n",
        "    print('Loss: ', np.array(history.history['loss']).mean())\n",
        "    print('Loss val: ', np.array(history.history['val_loss']).mean())\n",
        "    print('_'*50)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch size: 16\n",
            "__________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 21s 2ms/step - loss: 0.0969 - accuracy: 0.9615 - val_loss: 24.9863 - val_accuracy: 0.6932\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 20s 2ms/step - loss: 0.1808 - accuracy: 0.9604 - val_loss: 24.4766 - val_accuracy: 0.6870\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 20s 2ms/step - loss: 0.1277 - accuracy: 0.9607 - val_loss: 24.9549 - val_accuracy: 0.7119\n",
            "Loss:  0.1351121360283479\n",
            "Loss val:  24.80592831336769\n",
            "__________________________________________________\n",
            "batch size: 32\n",
            "__________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 10s 1ms/step - loss: 0.1239 - accuracy: 0.9605 - val_loss: 24.5799 - val_accuracy: 0.6866\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 10s 1ms/step - loss: 0.1005 - accuracy: 0.9621 - val_loss: 27.2295 - val_accuracy: 0.6687\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 10s 1ms/step - loss: 0.1214 - accuracy: 0.9619 - val_loss: 24.1321 - val_accuracy: 0.6834\n",
            "Loss:  0.1152912547603789\n",
            "Loss val:  25.31381204256356\n",
            "__________________________________________________\n",
            "batch size: 64\n",
            "__________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 6s 716us/step - loss: 0.1170 - accuracy: 0.9604 - val_loss: 24.1317 - val_accuracy: 0.6932\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 6s 709us/step - loss: 0.0862 - accuracy: 0.9635 - val_loss: 24.3067 - val_accuracy: 0.6901\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 6s 716us/step - loss: 0.0832 - accuracy: 0.9615 - val_loss: 25.9895 - val_accuracy: 0.6670\n",
            "Loss:  0.09547893341233955\n",
            "Loss val:  24.809292791858557\n",
            "__________________________________________________\n",
            "batch size: 128\n",
            "__________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 4s 497us/step - loss: 0.0817 - accuracy: 0.9627 - val_loss: 25.9994 - val_accuracy: 0.6883\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 5s 508us/step - loss: 0.0688 - accuracy: 0.9629 - val_loss: 25.2367 - val_accuracy: 0.6768\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 5s 508us/step - loss: 0.0667 - accuracy: 0.9611 - val_loss: 25.6927 - val_accuracy: 0.6665\n",
            "Loss:  0.07240790576168532\n",
            "Loss val:  25.6429401979704\n",
            "__________________________________________________\n",
            "batch size: 256\n",
            "__________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 4s 402us/step - loss: 0.0674 - accuracy: 0.9610 - val_loss: 26.4661 - val_accuracy: 0.6549\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 4s 397us/step - loss: 0.0640 - accuracy: 0.9606 - val_loss: 27.4433 - val_accuracy: 0.6656\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 4s 398us/step - loss: 0.1028 - accuracy: 0.9597 - val_loss: 24.5988 - val_accuracy: 0.6514\n",
            "Loss:  0.0780654756768991\n",
            "Loss val:  26.16939664191328\n",
            "__________________________________________________\n",
            "batch size: 512\n",
            "__________________________________________________\n",
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/3\n",
            "8982/8982 [==============================] - 3s 347us/step - loss: 0.0591 - accuracy: 0.9642 - val_loss: 25.5972 - val_accuracy: 0.6679\n",
            "Epoch 2/3\n",
            "8982/8982 [==============================] - 3s 343us/step - loss: 0.0549 - accuracy: 0.9619 - val_loss: 25.7059 - val_accuracy: 0.6705\n",
            "Epoch 3/3\n",
            "8982/8982 [==============================] - 3s 344us/step - loss: 0.0542 - accuracy: 0.9617 - val_loss: 25.9053 - val_accuracy: 0.6625\n",
            "Loss:  0.056078498533973974\n",
            "Loss val:  25.73613650921227\n",
            "__________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o597VJseX6cr",
        "colab_type": "code",
        "outputId": "a32495fc-3aa9-4436-8076-8a05751c3f13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot([16, 32, 64, 128, 256, 512], loss)\n",
        "plt.xlabel('batch size')\n",
        "plt.ylabel('loss');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIfg-6BiX6Za",
        "colab_type": "code",
        "outputId": "410dc8af-35fa-4b29-89bb-01117fda0872",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "plt.plot([16, 32, 64, 128, 256, 512], loss_val)\n",
        "plt.xlabel('batch size')\n",
        "plt.ylabel('loss_val');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv-XphNdmZlU",
        "colab_type": "text"
      },
      "source": [
        "## higher batch sizes leads to higher asymptotic val loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QaTzoaxHI7K",
        "colab_type": "text"
      },
      "source": [
        "# different regularization options (e.g. L2 and dropout)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xj8TcxMNHKoK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a6a79078-4179-499d-a1c7-568e73f37ad6"
      },
      "source": [
        "# consider default l1 regularization\n",
        "net_l1 = models.Sequential()\n",
        "net_l1.add(layers.Dense(units=300, activation='relu', input_shape=(10000,), kernel_regularizer='l1'))\n",
        "net_l1.add(layers.Dense(units=300, activation='relu', input_shape=(10000,), kernel_regularizer='l1'))\n",
        "net_l1.add(layers.Dense(units=46, activation='softmax'))\n",
        "\n",
        "net_l1.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy']) \n",
        "\n",
        "\n",
        "history_l1_default = net_l1.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=10, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/10\n",
            "8982/8982 [==============================] - 6s 712us/step - loss: 48.7210 - accuracy: 0.3692 - val_loss: 17.6749 - val_accuracy: 0.4893\n",
            "Epoch 2/10\n",
            "8982/8982 [==============================] - 6s 692us/step - loss: 16.7635 - accuracy: 0.4994 - val_loss: 16.4289 - val_accuracy: 0.4996\n",
            "Epoch 3/10\n",
            "8982/8982 [==============================] - 6s 695us/step - loss: 16.5166 - accuracy: 0.5118 - val_loss: 16.3822 - val_accuracy: 0.5249\n",
            "Epoch 4/10\n",
            "8982/8982 [==============================] - 6s 693us/step - loss: 16.4521 - accuracy: 0.5262 - val_loss: 16.3260 - val_accuracy: 0.5543\n",
            "Epoch 5/10\n",
            "8982/8982 [==============================] - 6s 694us/step - loss: 16.3977 - accuracy: 0.5563 - val_loss: 16.2737 - val_accuracy: 0.5646\n",
            "Epoch 6/10\n",
            "8982/8982 [==============================] - 6s 689us/step - loss: 16.3531 - accuracy: 0.5684 - val_loss: 16.2513 - val_accuracy: 0.5730\n",
            "Epoch 7/10\n",
            "8982/8982 [==============================] - 6s 690us/step - loss: 16.3265 - accuracy: 0.5725 - val_loss: 16.2186 - val_accuracy: 0.5757\n",
            "Epoch 8/10\n",
            "8982/8982 [==============================] - 6s 694us/step - loss: 16.3049 - accuracy: 0.5766 - val_loss: 16.2165 - val_accuracy: 0.5766\n",
            "Epoch 9/10\n",
            "8982/8982 [==============================] - 6s 691us/step - loss: 16.2922 - accuracy: 0.5790 - val_loss: 16.2088 - val_accuracy: 0.5828\n",
            "Epoch 10/10\n",
            "8982/8982 [==============================] - 6s 681us/step - loss: 16.2787 - accuracy: 0.5812 - val_loss: 16.1876 - val_accuracy: 0.5788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6ZLiLjWJ6hq",
        "colab_type": "text"
      },
      "source": [
        "## weak model performance, but we reduced owerfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJBa7hclHLcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "af3f77e3-c301-4303-97b5-184d34b5b570"
      },
      "source": [
        "# non default l1, try different parameters\n",
        "net_l1 = models.Sequential()\n",
        "net_l1.add(layers.Dense(units=300, activation='relu', input_shape=(10000,), kernel_regularizer=keras.regularizers.l1(0.01)))\n",
        "net_l1.add(layers.Dense(units=300, activation='relu', input_shape=(10000,), kernel_regularizer=keras.regularizers.l1(0.01)))\n",
        "net_l1.add(layers.Dense(units=46, activation='softmax'))\n",
        "\n",
        "net_l1.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy']) \n",
        "\n",
        "\n",
        "history_l1 = net_l1.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=10, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/10\n",
            "8982/8982 [==============================] - 6s 693us/step - loss: 48.6659 - accuracy: 0.3805 - val_loss: 17.6452 - val_accuracy: 0.5098\n",
            "Epoch 2/10\n",
            "8982/8982 [==============================] - 6s 681us/step - loss: 16.6187 - accuracy: 0.5089 - val_loss: 16.2332 - val_accuracy: 0.5223\n",
            "Epoch 3/10\n",
            "8982/8982 [==============================] - 6s 684us/step - loss: 16.3214 - accuracy: 0.5216 - val_loss: 16.1968 - val_accuracy: 0.5227\n",
            "Epoch 4/10\n",
            "8982/8982 [==============================] - 6s 680us/step - loss: 16.2916 - accuracy: 0.5328 - val_loss: 16.1770 - val_accuracy: 0.5668\n",
            "Epoch 5/10\n",
            "8982/8982 [==============================] - 6s 689us/step - loss: 16.2554 - accuracy: 0.5611 - val_loss: 16.1271 - val_accuracy: 0.5726\n",
            "Epoch 6/10\n",
            "8982/8982 [==============================] - 6s 674us/step - loss: 16.2139 - accuracy: 0.5703 - val_loss: 16.1055 - val_accuracy: 0.5708\n",
            "Epoch 7/10\n",
            "8982/8982 [==============================] - 6s 681us/step - loss: 16.1959 - accuracy: 0.5738 - val_loss: 16.1149 - val_accuracy: 0.5677\n",
            "Epoch 8/10\n",
            "8982/8982 [==============================] - 6s 683us/step - loss: 16.1908 - accuracy: 0.5748 - val_loss: 16.0954 - val_accuracy: 0.5766\n",
            "Epoch 9/10\n",
            "8982/8982 [==============================] - 6s 670us/step - loss: 16.1783 - accuracy: 0.5785 - val_loss: 16.0729 - val_accuracy: 0.5819\n",
            "Epoch 10/10\n",
            "8982/8982 [==============================] - 6s 671us/step - loss: 16.1595 - accuracy: 0.5821 - val_loss: 16.0632 - val_accuracy: 0.5801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcrWLw5JKE-R",
        "colab_type": "text"
      },
      "source": [
        "## model still acts the same, try l2 regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb1CoJGTHLad",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "8d38bb29-a791-448f-9409-07372a929b78"
      },
      "source": [
        "# default l2\n",
        "net_l2 = models.Sequential()\n",
        "net_l2.add(layers.Dense(units=300, activation='relu', input_shape=(10000,), kernel_regularizer='l2'))\n",
        "net_l2.add(layers.Dense(units=300, activation='relu', input_shape=(10000,), kernel_regularizer='l2'))\n",
        "net_l2.add(layers.Dense(units=46, activation='softmax'))\n",
        "\n",
        "net_l2.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy']) \n",
        "\n",
        "\n",
        "history_l2_default = net_l2.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=10, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/10\n",
            "8982/8982 [==============================] - 6s 670us/step - loss: 3.2601 - accuracy: 0.6602 - val_loss: 1.9385 - val_accuracy: 0.7213\n",
            "Epoch 2/10\n",
            "8982/8982 [==============================] - 6s 644us/step - loss: 1.6879 - accuracy: 0.7400 - val_loss: 1.6870 - val_accuracy: 0.7026\n",
            "Epoch 3/10\n",
            "8982/8982 [==============================] - 6s 642us/step - loss: 1.4996 - accuracy: 0.7590 - val_loss: 1.5193 - val_accuracy: 0.7413\n",
            "Epoch 4/10\n",
            "8982/8982 [==============================] - 6s 646us/step - loss: 1.4158 - accuracy: 0.7685 - val_loss: 1.4896 - val_accuracy: 0.7502\n",
            "Epoch 5/10\n",
            "8982/8982 [==============================] - 6s 641us/step - loss: 1.3672 - accuracy: 0.7739 - val_loss: 1.4658 - val_accuracy: 0.7311\n",
            "Epoch 6/10\n",
            "8982/8982 [==============================] - 6s 651us/step - loss: 1.3135 - accuracy: 0.7808 - val_loss: 1.4309 - val_accuracy: 0.7480\n",
            "Epoch 7/10\n",
            "8982/8982 [==============================] - 6s 648us/step - loss: 1.2625 - accuracy: 0.7918 - val_loss: 1.3823 - val_accuracy: 0.7591\n",
            "Epoch 8/10\n",
            "8982/8982 [==============================] - 6s 645us/step - loss: 1.2207 - accuracy: 0.7987 - val_loss: 1.3992 - val_accuracy: 0.7582\n",
            "Epoch 9/10\n",
            "8982/8982 [==============================] - 6s 634us/step - loss: 1.1848 - accuracy: 0.8043 - val_loss: 1.3451 - val_accuracy: 0.7578\n",
            "Epoch 10/10\n",
            "8982/8982 [==============================] - 6s 640us/step - loss: 1.1522 - accuracy: 0.8079 - val_loss: 1.3299 - val_accuracy: 0.7703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7ABtJTdKhCs",
        "colab_type": "text"
      },
      "source": [
        "## l2 works much better and still reduce overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNZ2kwQpHLWw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "170e0503-6848-4878-d3fe-19b0dd75504a"
      },
      "source": [
        "# try non default l2\n",
        "net_l2 = models.Sequential()\n",
        "net_l2.add(layers.Dense(units=300, activation='relu', input_shape=(10000,), kernel_regularizer=keras.regularizers.l2(0.01)))\n",
        "net_l2.add(layers.Dense(units=300, activation='relu', input_shape=(10000,), kernel_regularizer=keras.regularizers.l2(0.01)))\n",
        "net_l2.add(layers.Dense(units=46, activation='softmax'))\n",
        "\n",
        "net_l2.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy']) \n",
        "\n",
        "\n",
        "history_l2 = net_l2.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=10, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/10\n",
            "8982/8982 [==============================] - 6s 672us/step - loss: 3.2220 - accuracy: 0.6618 - val_loss: 1.9600 - val_accuracy: 0.7088\n",
            "Epoch 2/10\n",
            "8982/8982 [==============================] - 6s 662us/step - loss: 1.6873 - accuracy: 0.7393 - val_loss: 1.6525 - val_accuracy: 0.7346\n",
            "Epoch 3/10\n",
            "8982/8982 [==============================] - 6s 653us/step - loss: 1.4946 - accuracy: 0.7575 - val_loss: 1.5580 - val_accuracy: 0.7324\n",
            "Epoch 4/10\n",
            "8982/8982 [==============================] - 6s 650us/step - loss: 1.4058 - accuracy: 0.7711 - val_loss: 1.4868 - val_accuracy: 0.7458\n",
            "Epoch 5/10\n",
            "8982/8982 [==============================] - 6s 648us/step - loss: 1.3471 - accuracy: 0.7794 - val_loss: 1.4647 - val_accuracy: 0.7386\n",
            "Epoch 6/10\n",
            "8982/8982 [==============================] - 6s 646us/step - loss: 1.3011 - accuracy: 0.7870 - val_loss: 1.4373 - val_accuracy: 0.7573\n",
            "Epoch 7/10\n",
            "8982/8982 [==============================] - 6s 654us/step - loss: 1.2537 - accuracy: 0.7940 - val_loss: 1.3789 - val_accuracy: 0.7524\n",
            "Epoch 8/10\n",
            "8982/8982 [==============================] - 6s 653us/step - loss: 1.2175 - accuracy: 0.8013 - val_loss: 1.3542 - val_accuracy: 0.7578\n",
            "Epoch 9/10\n",
            "8982/8982 [==============================] - 6s 648us/step - loss: 1.1738 - accuracy: 0.8064 - val_loss: 1.3273 - val_accuracy: 0.7658\n",
            "Epoch 10/10\n",
            "8982/8982 [==============================] - 6s 652us/step - loss: 1.1359 - accuracy: 0.8142 - val_loss: 1.3332 - val_accuracy: 0.7600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7k-TgNAhLifA",
        "colab_type": "text"
      },
      "source": [
        "## experiment with dropout"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQDyTu_rHLVJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a9a66bbb-dc20-4f1c-e4e5-9224171fe1e0"
      },
      "source": [
        "# dropout 0.25\n",
        "net_drop = models.Sequential()\n",
        "net_drop.add(layers.Dense(units=300, activation='relu', input_shape=(10000,)))\n",
        "net_drop.add(layers.Dense(units=300, activation='relu', input_shape=(10000,)))\n",
        "net_drop.add(layers.Dropout(0.25))\n",
        "net_drop.add(layers.Dense(units=46, activation='softmax'))\n",
        "\n",
        "net_drop.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy']) \n",
        "\n",
        "\n",
        "history_drop = net_drop.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=10, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/10\n",
            "8982/8982 [==============================] - 5s 582us/step - loss: 1.3485 - accuracy: 0.7005 - val_loss: 1.0017 - val_accuracy: 0.7658\n",
            "Epoch 2/10\n",
            "8982/8982 [==============================] - 5s 563us/step - loss: 0.6190 - accuracy: 0.8641 - val_loss: 0.9018 - val_accuracy: 0.7988\n",
            "Epoch 3/10\n",
            "8982/8982 [==============================] - 5s 563us/step - loss: 0.3486 - accuracy: 0.9225 - val_loss: 0.9139 - val_accuracy: 0.8126\n",
            "Epoch 4/10\n",
            "8982/8982 [==============================] - 5s 570us/step - loss: 0.2443 - accuracy: 0.9413 - val_loss: 1.0662 - val_accuracy: 0.8014\n",
            "Epoch 5/10\n",
            "8982/8982 [==============================] - 5s 564us/step - loss: 0.1982 - accuracy: 0.9471 - val_loss: 1.1126 - val_accuracy: 0.7983\n",
            "Epoch 6/10\n",
            "8982/8982 [==============================] - 5s 558us/step - loss: 0.1686 - accuracy: 0.9491 - val_loss: 1.3014 - val_accuracy: 0.7939\n",
            "Epoch 7/10\n",
            "8982/8982 [==============================] - 5s 568us/step - loss: 0.1543 - accuracy: 0.9510 - val_loss: 1.4093 - val_accuracy: 0.7845\n",
            "Epoch 8/10\n",
            "8982/8982 [==============================] - 5s 568us/step - loss: 0.1396 - accuracy: 0.9525 - val_loss: 1.4461 - val_accuracy: 0.8010\n",
            "Epoch 9/10\n",
            "8982/8982 [==============================] - 5s 561us/step - loss: 0.1263 - accuracy: 0.9531 - val_loss: 1.5094 - val_accuracy: 0.7943\n",
            "Epoch 10/10\n",
            "8982/8982 [==============================] - 5s 564us/step - loss: 0.1198 - accuracy: 0.9526 - val_loss: 1.6548 - val_accuracy: 0.7988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKdzlogWHLTg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "a53240d3-b6ee-4009-f6c9-4bcccd662cf4"
      },
      "source": [
        "# dropout 0.5\n",
        "net_drop = models.Sequential()\n",
        "net_drop.add(layers.Dense(units=300, activation='relu', input_shape=(10000,)))\n",
        "net_drop.add(layers.Dense(units=300, activation='relu', input_shape=(10000,)))\n",
        "net_drop.add(layers.Dropout(0.5))\n",
        "net_drop.add(layers.Dense(units=46, activation='softmax'))\n",
        "\n",
        "net_drop.compile(loss='categorical_crossentropy',\n",
        "                optimizer='rmsprop',\n",
        "                metrics=['accuracy']) \n",
        "\n",
        "\n",
        "history_drop = net_drop.fit(train_features, \n",
        "                      train_target,\n",
        "                      epochs=10, \n",
        "                      verbose=1,\n",
        "                      batch_size=100, \n",
        "                      validation_data=(test_features, test_target))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 8982 samples, validate on 2246 samples\n",
            "Epoch 1/10\n",
            "8982/8982 [==============================] - 5s 572us/step - loss: 1.4447 - accuracy: 0.6780 - val_loss: 1.0548 - val_accuracy: 0.7578\n",
            "Epoch 2/10\n",
            "8982/8982 [==============================] - 5s 561us/step - loss: 0.7329 - accuracy: 0.8321 - val_loss: 0.9314 - val_accuracy: 0.7867\n",
            "Epoch 3/10\n",
            "8982/8982 [==============================] - 5s 559us/step - loss: 0.4463 - accuracy: 0.8958 - val_loss: 0.9146 - val_accuracy: 0.8037\n",
            "Epoch 4/10\n",
            "8982/8982 [==============================] - 5s 557us/step - loss: 0.3020 - accuracy: 0.9284 - val_loss: 1.0050 - val_accuracy: 0.7996\n",
            "Epoch 5/10\n",
            "8982/8982 [==============================] - 5s 554us/step - loss: 0.2280 - accuracy: 0.9417 - val_loss: 1.1187 - val_accuracy: 0.8023\n",
            "Epoch 6/10\n",
            "8982/8982 [==============================] - 5s 561us/step - loss: 0.1943 - accuracy: 0.9487 - val_loss: 1.2580 - val_accuracy: 0.7952\n",
            "Epoch 7/10\n",
            "8982/8982 [==============================] - 5s 548us/step - loss: 0.1709 - accuracy: 0.9516 - val_loss: 1.3357 - val_accuracy: 0.7943\n",
            "Epoch 8/10\n",
            "8982/8982 [==============================] - 5s 563us/step - loss: 0.1536 - accuracy: 0.9522 - val_loss: 1.5261 - val_accuracy: 0.7952\n",
            "Epoch 9/10\n",
            "8982/8982 [==============================] - 5s 559us/step - loss: 0.1452 - accuracy: 0.9520 - val_loss: 1.6008 - val_accuracy: 0.7939\n",
            "Epoch 10/10\n",
            "8982/8982 [==============================] - 5s 558us/step - loss: 0.1325 - accuracy: 0.9548 - val_loss: 1.7298 - val_accuracy: 0.7934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFJ1arIuMZZF",
        "colab_type": "text"
      },
      "source": [
        "## regularization using dropout performs slightly better than l1 and l2"
      ]
    }
  ]
}