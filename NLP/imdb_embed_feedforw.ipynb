{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of imdb_embed_feedforw.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUObTuKea9xy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKPHVl2drIJR",
        "colab_type": "code",
        "outputId": "4ea26331-cd9a-429f-93e3-2284c90d3d4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import preprocessing\n",
        "max_features = 10000\n",
        "maxlen = 200\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) \n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkqBll-NrIFL",
        "colab_type": "code",
        "outputId": "b456d038-ac78-48bd-b945-2a2abb92dc00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Flatten, Dense, Embedding\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, 32, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 200, 32)           320000    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6400)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 6401      \n",
            "=================================================================\n",
            "Total params: 326,401\n",
            "Trainable params: 326,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/10\n",
            "20000/20000 [==============================] - 4s 179us/step - loss: 0.4864 - acc: 0.7715 - val_loss: 0.3140 - val_acc: 0.8740\n",
            "Epoch 2/10\n",
            "20000/20000 [==============================] - 2s 88us/step - loss: 0.2442 - acc: 0.9035 - val_loss: 0.2841 - val_acc: 0.8838\n",
            "Epoch 3/10\n",
            "20000/20000 [==============================] - 2s 91us/step - loss: 0.1800 - acc: 0.9327 - val_loss: 0.2864 - val_acc: 0.8820\n",
            "Epoch 4/10\n",
            "20000/20000 [==============================] - 2s 92us/step - loss: 0.1320 - acc: 0.9535 - val_loss: 0.2917 - val_acc: 0.8834\n",
            "Epoch 5/10\n",
            "20000/20000 [==============================] - 2s 88us/step - loss: 0.0915 - acc: 0.9709 - val_loss: 0.3108 - val_acc: 0.8812\n",
            "Epoch 6/10\n",
            "20000/20000 [==============================] - 2s 90us/step - loss: 0.0570 - acc: 0.9840 - val_loss: 0.3446 - val_acc: 0.8732\n",
            "Epoch 7/10\n",
            "20000/20000 [==============================] - 2s 88us/step - loss: 0.0330 - acc: 0.9927 - val_loss: 0.3536 - val_acc: 0.8738\n",
            "Epoch 8/10\n",
            "20000/20000 [==============================] - 2s 88us/step - loss: 0.0173 - acc: 0.9965 - val_loss: 0.3860 - val_acc: 0.8738\n",
            "Epoch 9/10\n",
            "20000/20000 [==============================] - 2s 88us/step - loss: 0.0090 - acc: 0.9984 - val_loss: 0.4280 - val_acc: 0.8742\n",
            "Epoch 10/10\n",
            "20000/20000 [==============================] - 2s 91us/step - loss: 0.0045 - acc: 0.9989 - val_loss: 0.4585 - val_acc: 0.8738\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbv5MWkQ6gyz",
        "colab_type": "code",
        "outputId": "5aef4b4c-219c-4382-a5e6-32eb1233842d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-27 13:32:11--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  61.5MB/s    in 1.3s    \n",
            "\n",
            "2020-04-27 13:32:12 (61.5 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5Uj5p6860vp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tar --gunzip --extract --file=aclImdb_v1.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4l1c_AS7P7G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "imdb_dir = '/content/aclImdb'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfDMu-OdrICt",
        "colab_type": "code",
        "outputId": "f3220549-c85e-4677-e3d0-d2fab561cef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "maxlen = 300\n",
        "max_words = 10000 \n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "\n",
        "nb_validation_samples = int(0.1 * data.shape[0])\n",
        "x_train = data[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "x_val = data[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]\n",
        "#x_train = data[:training_samples]\n",
        "#y_train = labels[:training_samples]\n",
        "#x_val = data[training_samples: training_samples + validation_samples]\n",
        "#y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 88582 unique tokens.\n",
            "Shape of data tensor: (25000, 300)\n",
            "Shape of label tensor: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc8D2UYr_XGR",
        "colab_type": "code",
        "outputId": "90f5eeab-fe1f-4305-f13a-3ef2c0657431",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-27 06:09:07--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-04-27 06:09:07--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-04-27 06:09:07--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  1.91MB/s    in 6m 27s  \n",
            "\n",
            "2020-04-27 06:15:34 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLymZZu6BkKc",
        "colab_type": "code",
        "outputId": "02d530ec-7510-4666-e2aa-1601d31308b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!unzip glove.6B.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F2E0149rH6b",
        "colab_type": "code",
        "outputId": "e07ffb48-ad59-4526-a113-7c1fcfd73b89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "glove_dir = '/content'\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3WSh23trH4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tclWjVZhrHx_",
        "colab_type": "code",
        "outputId": "86dd2005-b8c3-4f20-8e8d-f53634556ec4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense\n",
        "model_embed = Sequential()\n",
        "model_embed.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model_embed.add(Flatten())\n",
        "model_embed.add(Dense(32, activation='relu'))\n",
        "model_embed.add(Dense(1, activation='sigmoid'))\n",
        "model_embed.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_8 (Embedding)      (None, 300, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 30000)             0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 32)                960032    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,960,065\n",
            "Trainable params: 1,960,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbtXH9WKrHtF",
        "colab_type": "code",
        "outputId": "701cc9bf-b867-4f49-d076-f6eedb2caaaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "model_embed.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history_embed = model_embed.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "model_embed.save_weights('embed_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 22500 samples, validate on 2500 samples\n",
            "Epoch 1/10\n",
            "22500/22500 [==============================] - 2s 107us/step - loss: 0.4076 - acc: 0.7980 - val_loss: 0.2915 - val_acc: 0.8704\n",
            "Epoch 2/10\n",
            "22500/22500 [==============================] - 2s 101us/step - loss: 0.1043 - acc: 0.9613 - val_loss: 0.4147 - val_acc: 0.8540\n",
            "Epoch 3/10\n",
            "22500/22500 [==============================] - 2s 102us/step - loss: 0.0125 - acc: 0.9961 - val_loss: 0.6003 - val_acc: 0.8476\n",
            "Epoch 4/10\n",
            "22500/22500 [==============================] - 2s 102us/step - loss: 0.0011 - acc: 0.9997 - val_loss: 0.8303 - val_acc: 0.8448\n",
            "Epoch 5/10\n",
            "22500/22500 [==============================] - 2s 102us/step - loss: 2.2804e-04 - acc: 1.0000 - val_loss: 1.0382 - val_acc: 0.8380\n",
            "Epoch 6/10\n",
            "22500/22500 [==============================] - 2s 100us/step - loss: 8.5992e-06 - acc: 1.0000 - val_loss: 1.1974 - val_acc: 0.8340\n",
            "Epoch 7/10\n",
            "22500/22500 [==============================] - 2s 103us/step - loss: 2.8415e-08 - acc: 1.0000 - val_loss: 1.2867 - val_acc: 0.8376\n",
            "Epoch 8/10\n",
            "22500/22500 [==============================] - 2s 103us/step - loss: 2.7719e-09 - acc: 1.0000 - val_loss: 1.3113 - val_acc: 0.8360\n",
            "Epoch 9/10\n",
            "22500/22500 [==============================] - 2s 105us/step - loss: 2.3137e-09 - acc: 1.0000 - val_loss: 1.3141 - val_acc: 0.8388\n",
            "Epoch 10/10\n",
            "22500/22500 [==============================] - 2s 105us/step - loss: 1.3060e-09 - acc: 1.0000 - val_loss: 1.3215 - val_acc: 0.8376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UsdURulShHf",
        "colab_type": "code",
        "outputId": "5ff5e4a1-7a2c-482d-a338-9cd5c6b486dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "model_embed_pretrain = Sequential()\n",
        "model_embed_pretrain.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model_embed_pretrain.add(Flatten())\n",
        "model_embed_pretrain.add(Dense(32, activation='relu'))\n",
        "model_embed_pretrain.add(Dense(1, activation='sigmoid'))\n",
        "model_embed_pretrain.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_9 (Embedding)      (None, 300, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 30000)             0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 32)                960032    \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,960,065\n",
            "Trainable params: 1,960,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FtXRXvHirHwC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_embed_pretrain.layers[0].set_weights([embedding_matrix])\n",
        "model_embed_pretrain.layers[0].trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMFaceDXTEHc",
        "colab_type": "code",
        "outputId": "d867ebae-2ec0-4f2d-d4c8-db8c312e8361",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "model_embed_pretrain.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history_embed_pretrain = model_embed_pretrain.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "model_embed.save_weights('embed_pretrain_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 22500 samples, validate on 2500 samples\n",
            "Epoch 1/10\n",
            "22500/22500 [==============================] - 2s 87us/step - loss: 0.6552 - acc: 0.6397 - val_loss: 0.5733 - val_acc: 0.7156\n",
            "Epoch 2/10\n",
            "22500/22500 [==============================] - 2s 80us/step - loss: 0.4916 - acc: 0.7667 - val_loss: 0.5729 - val_acc: 0.7388\n",
            "Epoch 3/10\n",
            "22500/22500 [==============================] - 2s 80us/step - loss: 0.3837 - acc: 0.8250 - val_loss: 0.8569 - val_acc: 0.6432\n",
            "Epoch 4/10\n",
            "22500/22500 [==============================] - 2s 80us/step - loss: 0.3033 - acc: 0.8665 - val_loss: 0.6895 - val_acc: 0.7256\n",
            "Epoch 5/10\n",
            "22500/22500 [==============================] - 2s 81us/step - loss: 0.2316 - acc: 0.8992 - val_loss: 1.0159 - val_acc: 0.7120\n",
            "Epoch 6/10\n",
            "22500/22500 [==============================] - 2s 88us/step - loss: 0.1761 - acc: 0.9248 - val_loss: 1.1324 - val_acc: 0.6816\n",
            "Epoch 7/10\n",
            "22500/22500 [==============================] - 2s 87us/step - loss: 0.1356 - acc: 0.9435 - val_loss: 1.1270 - val_acc: 0.7204\n",
            "Epoch 8/10\n",
            "22500/22500 [==============================] - 2s 85us/step - loss: 0.1013 - acc: 0.9576 - val_loss: 1.2625 - val_acc: 0.6884\n",
            "Epoch 9/10\n",
            "22500/22500 [==============================] - 2s 88us/step - loss: 0.0778 - acc: 0.9687 - val_loss: 1.3312 - val_acc: 0.7120\n",
            "Epoch 10/10\n",
            "22500/22500 [==============================] - 2s 86us/step - loss: 0.0660 - acc: 0.9736 - val_loss: 1.4863 - val_acc: 0.7236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqHxaIOErHps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U_wxzRkrHoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname))\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiT_UOlorHlT",
        "colab_type": "code",
        "outputId": "742ee102-0e5a-4c02-9ceb-3b81ffa99bfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#model_embed.load_weights('embed_model.h5')\n",
        "model_embed.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 44us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3356188513851166, 0.8370800018310547]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBmiA2v5rHiP",
        "colab_type": "code",
        "outputId": "13ca3f9f-370f-4119-f243-9ec492f36fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#model_embed_pretrain.load_weights('embed_pretrain_model.h5')\n",
        "model_embed_pretrain.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 45us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.4055371337699891, 0.7221599817276001]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU7cQOwZrEQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OSXCczLrENU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwqO3AVWrELT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "max_features = 10000\n",
        "maxlen = 500\n",
        "batch_size = 32\n",
        "print('Loading data...')\n",
        "(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(input_train), 'train sequences')\n",
        "print(len(input_test), 'test sequences')\n",
        "print('Pad sequences (samples x time)')\n",
        "input_train = sequence.pad_sequences(input_train, maxlen=maxlen)\n",
        "input_test = sequence.pad_sequences(input_test, maxlen=maxlen)\n",
        "print('input_train shape:', input_train.shape)\n",
        "print('input_test shape:', input_test.shape)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1X36OjNrEIQ",
        "colab_type": "code",
        "outputId": "f565ca5c-71f5-4163-962d-274b314449fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN, Dense\n",
        "model_simple_rnn = Sequential()\n",
        "model_simple_rnn.add(Embedding(10000, 32))\n",
        "model_simple_rnn.add(SimpleRNN(32))\n",
        "model_simple_rnn.add(Dense(1, activation='sigmoid'))\n",
        "model_simple_rnn.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history_simple_rnn = model_simple_rnn.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "model_simple_rnn.save_weights('simple_rnn_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 22500 samples, validate on 2500 samples\n",
            "Epoch 1/10\n",
            "22500/22500 [==============================] - 137s 6ms/step - loss: 0.5632 - acc: 0.6829 - val_loss: 0.9725 - val_acc: 0.6332\n",
            "Epoch 2/10\n",
            "22500/22500 [==============================] - 135s 6ms/step - loss: 0.3654 - acc: 0.8477 - val_loss: 0.4160 - val_acc: 0.8216\n",
            "Epoch 3/10\n",
            "22500/22500 [==============================] - 135s 6ms/step - loss: 0.3073 - acc: 0.8780 - val_loss: 0.3894 - val_acc: 0.8256\n",
            "Epoch 4/10\n",
            "22500/22500 [==============================] - 133s 6ms/step - loss: 0.2608 - acc: 0.8956 - val_loss: 0.3411 - val_acc: 0.8712\n",
            "Epoch 5/10\n",
            "22500/22500 [==============================] - 131s 6ms/step - loss: 0.2284 - acc: 0.9118 - val_loss: 0.3743 - val_acc: 0.8328\n",
            "Epoch 6/10\n",
            "22500/22500 [==============================] - 133s 6ms/step - loss: 0.1947 - acc: 0.9255 - val_loss: 0.3567 - val_acc: 0.8656\n",
            "Epoch 7/10\n",
            "22500/22500 [==============================] - 134s 6ms/step - loss: 0.1725 - acc: 0.9353 - val_loss: 0.3740 - val_acc: 0.8588\n",
            "Epoch 8/10\n",
            "22500/22500 [==============================] - 132s 6ms/step - loss: 0.1473 - acc: 0.9470 - val_loss: 0.5326 - val_acc: 0.8432\n",
            "Epoch 9/10\n",
            "22500/22500 [==============================] - 129s 6ms/step - loss: 0.1223 - acc: 0.9552 - val_loss: 0.4573 - val_acc: 0.8112\n",
            "Epoch 10/10\n",
            "22500/22500 [==============================] - 129s 6ms/step - loss: 0.0983 - acc: 0.9636 - val_loss: 0.4762 - val_acc: 0.8416\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbGzNnpiWUX6",
        "colab_type": "code",
        "outputId": "24b4d181-c7fb-424b-d267-27b661684427",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#model_simple_rnn.load_weights('simple_rnn_model.h5')\n",
        "model_simple_rnn.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 14s 560us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4910227665400505, 0.8416399955749512]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q93FwimWrEED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv_P3dfBrEAL",
        "colab_type": "code",
        "outputId": "e6025555-b59f-4c29-e17a-11f7e0d5ddab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "from keras.layers import LSTM\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(Embedding(10000, 32))\n",
        "model_lstm.add(LSTM(32))\n",
        "model_lstm.add(Dense(1, activation='sigmoid'))\n",
        "model_lstm.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "history_lstm = model_lstm.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
        "model_lstm.save_weights('lstm_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 22500 samples, validate on 2500 samples\n",
            "Epoch 1/10\n",
            "22500/22500 [==============================] - 337s 15ms/step - loss: 0.4098 - acc: 0.8152 - val_loss: 0.4102 - val_acc: 0.8164\n",
            "Epoch 2/10\n",
            "22500/22500 [==============================] - 339s 15ms/step - loss: 0.2594 - acc: 0.9008 - val_loss: 0.4270 - val_acc: 0.8316\n",
            "Epoch 3/10\n",
            "22500/22500 [==============================] - 336s 15ms/step - loss: 0.2183 - acc: 0.9165 - val_loss: 0.3317 - val_acc: 0.8880\n",
            "Epoch 4/10\n",
            "22500/22500 [==============================] - 337s 15ms/step - loss: 0.1944 - acc: 0.9270 - val_loss: 0.2722 - val_acc: 0.8916\n",
            "Epoch 5/10\n",
            "22500/22500 [==============================] - 335s 15ms/step - loss: 0.1808 - acc: 0.9348 - val_loss: 0.2620 - val_acc: 0.8896\n",
            "Epoch 6/10\n",
            "22500/22500 [==============================] - 344s 15ms/step - loss: 0.1668 - acc: 0.9390 - val_loss: 0.2869 - val_acc: 0.8852\n",
            "Epoch 7/10\n",
            "22500/22500 [==============================] - 353s 16ms/step - loss: 0.1556 - acc: 0.9442 - val_loss: 0.3072 - val_acc: 0.8820\n",
            "Epoch 8/10\n",
            "22500/22500 [==============================] - 357s 16ms/step - loss: 0.1434 - acc: 0.9495 - val_loss: 0.2820 - val_acc: 0.8888\n",
            "Epoch 9/10\n",
            "22500/22500 [==============================] - 354s 16ms/step - loss: 0.1354 - acc: 0.9509 - val_loss: 0.3048 - val_acc: 0.8856\n",
            "Epoch 10/10\n",
            "22500/22500 [==============================] - 353s 16ms/step - loss: 0.1253 - acc: 0.9566 - val_loss: 0.3207 - val_acc: 0.8884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDAohdlqW7Zk",
        "colab_type": "code",
        "outputId": "061acf8a-88c8-4c49-b203-f0ffa8bbeba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#model_lstm.load_weights('lstm_model.h5')\n",
        "model_lstm.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 33s 1ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.381427589725256, 0.8727999925613403]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzSAaiarrD92",
        "colab_type": "code",
        "outputId": "3dcf6a66-76b6-4f56-d573-dd9e14c7a14b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
        "from keras.datasets import imdb\n",
        "\n",
        "\n",
        "max_features = 10000\n",
        "maxlen = 300\n",
        "\n",
        "model_bilstm = Sequential()\n",
        "model_bilstm.add(Embedding(max_features, 32))\n",
        "model_bilstm.add(Bidirectional(LSTM(32)))\n",
        "#model_bilstm.add(Dropout(0.5))\n",
        "model_bilstm.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_bilstm.summary()\n",
        "\n",
        "model_bilstm.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history_bilstm = model_bilstm.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_val, y_val))\n",
        "model_bilstm.save_weights('bilstm_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_13 (Embedding)     (None, 300, 32)           320000    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 64)                16640     \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 336,705\n",
            "Trainable params: 336,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 22500 samples, validate on 2500 samples\n",
            "Epoch 1/20\n",
            "22500/22500 [==============================] - 658s 29ms/step - loss: 0.4182 - acc: 0.8103 - val_loss: 0.3094 - val_acc: 0.8808\n",
            "Epoch 2/20\n",
            "22500/22500 [==============================] - 635s 28ms/step - loss: 0.2586 - acc: 0.9011 - val_loss: 0.3562 - val_acc: 0.8540\n",
            "Epoch 3/20\n",
            "22500/22500 [==============================] - 646s 29ms/step - loss: 0.2196 - acc: 0.9181 - val_loss: 0.3160 - val_acc: 0.8776\n",
            "Epoch 4/20\n",
            "22500/22500 [==============================] - 674s 30ms/step - loss: 0.1950 - acc: 0.9286 - val_loss: 0.3137 - val_acc: 0.8932\n",
            "Epoch 5/20\n",
            "22500/22500 [==============================] - 663s 29ms/step - loss: 0.1798 - acc: 0.9344 - val_loss: 0.2816 - val_acc: 0.8808\n",
            "Epoch 6/20\n",
            "22500/22500 [==============================] - 618s 27ms/step - loss: 0.1618 - acc: 0.9404 - val_loss: 0.2919 - val_acc: 0.8928\n",
            "Epoch 7/20\n",
            "22500/22500 [==============================] - 619s 28ms/step - loss: 0.1511 - acc: 0.9449 - val_loss: 0.3006 - val_acc: 0.8944\n",
            "Epoch 8/20\n",
            "22500/22500 [==============================] - 645s 29ms/step - loss: 0.1437 - acc: 0.9485 - val_loss: 0.3056 - val_acc: 0.8888\n",
            "Epoch 9/20\n",
            "22500/22500 [==============================] - 661s 29ms/step - loss: 0.1321 - acc: 0.9536 - val_loss: 0.3098 - val_acc: 0.8920\n",
            "Epoch 10/20\n",
            "22500/22500 [==============================] - 662s 29ms/step - loss: 0.1214 - acc: 0.9579 - val_loss: 0.3443 - val_acc: 0.8836\n",
            "Epoch 11/20\n",
            "22500/22500 [==============================] - 662s 29ms/step - loss: 0.1140 - acc: 0.9590 - val_loss: 0.3653 - val_acc: 0.8652\n",
            "Epoch 12/20\n",
            "22500/22500 [==============================] - 649s 29ms/step - loss: 0.1077 - acc: 0.9639 - val_loss: 0.3175 - val_acc: 0.8828\n",
            "Epoch 13/20\n",
            "22500/22500 [==============================] - 655s 29ms/step - loss: 0.1017 - acc: 0.9658 - val_loss: 0.3385 - val_acc: 0.8832\n",
            "Epoch 14/20\n",
            "22500/22500 [==============================] - 660s 29ms/step - loss: 0.0952 - acc: 0.9684 - val_loss: 0.4118 - val_acc: 0.8676\n",
            "Epoch 15/20\n",
            "22500/22500 [==============================] - 681s 30ms/step - loss: 0.0886 - acc: 0.9697 - val_loss: 0.3920 - val_acc: 0.8800\n",
            "Epoch 16/20\n",
            "22500/22500 [==============================] - 665s 30ms/step - loss: 0.0779 - acc: 0.9735 - val_loss: 0.3963 - val_acc: 0.8768\n",
            "Epoch 17/20\n",
            "22500/22500 [==============================] - 662s 29ms/step - loss: 0.0723 - acc: 0.9761 - val_loss: 0.4054 - val_acc: 0.8588\n",
            "Epoch 18/20\n",
            "22500/22500 [==============================] - 623s 28ms/step - loss: 0.0644 - acc: 0.9788 - val_loss: 0.5286 - val_acc: 0.8696\n",
            "Epoch 19/20\n",
            "22500/22500 [==============================] - 630s 28ms/step - loss: 0.0575 - acc: 0.9813 - val_loss: 0.3879 - val_acc: 0.8764\n",
            "Epoch 20/20\n",
            "22500/22500 [==============================] - 615s 27ms/step - loss: 0.0504 - acc: 0.9835 - val_loss: 0.4954 - val_acc: 0.8688\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBY0Kl8KX3UH",
        "colab_type": "code",
        "outputId": "ae3a5df9-ec86-4867-df51-dd7d9ce31a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#model_bilstm.load_weights('bilstm_model.h5')\n",
        "model_bilstm.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 60s 2ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5186398750025034, 0.8530399799346924]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYykhVQEJLNv",
        "colab_type": "code",
        "outputId": "16907b9f-4f97-4523-804d-5d606517d80e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        }
      },
      "source": [
        "model_bilstm_pretrain = Sequential()\n",
        "#model_bilstm_pretrain.add(Embedding(10000, 100, input_length=300))\n",
        "model_bilstm_pretrain.add(Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1], input_length = 300, weights = [embedding_matrix], trainable=False))\n",
        "model_bilstm_pretrain.add(Bidirectional(LSTM(50)))\n",
        "#model_bilstm_pretrain.add(Dropout(0.25))\n",
        "#model_bilstm_pretrain.add(Dense(64))\n",
        "#model_bilstm_pretrain.add(Dropout(0.5))\n",
        "model_bilstm_pretrain.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "#model_bilstm_pretrain.layers[0].set_weights([embedding_matrix])\n",
        "#model_bilstm_pretrain.layers[0].trainable = False\n",
        "model_bilstm_pretrain.summary()\n",
        "\n",
        "model_bilstm_pretrain.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history_bilstm_pretrain = model_bilstm_pretrain.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_val, y_val))\n",
        "model_bilstm_pretrain.save_weights('bilstm_pretrain_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_14 (Embedding)     (None, 300, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dense_23 (Dense)             (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 1,060,501\n",
            "Trainable params: 60,501\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n",
            "Train on 22500 samples, validate on 2500 samples\n",
            "Epoch 1/10\n",
            "22500/22500 [==============================] - 152s 7ms/step - loss: 0.6075 - acc: 0.6697 - val_loss: 0.5358 - val_acc: 0.7412\n",
            "Epoch 2/10\n",
            "22500/22500 [==============================] - 147s 7ms/step - loss: 0.5282 - acc: 0.7443 - val_loss: 0.5602 - val_acc: 0.7412\n",
            "Epoch 3/10\n",
            "22500/22500 [==============================] - 150s 7ms/step - loss: 0.4687 - acc: 0.7813 - val_loss: 0.4550 - val_acc: 0.7908\n",
            "Epoch 4/10\n",
            "22500/22500 [==============================] - 148s 7ms/step - loss: 0.4267 - acc: 0.8079 - val_loss: 0.4435 - val_acc: 0.8008\n",
            "Epoch 5/10\n",
            "22500/22500 [==============================] - 147s 7ms/step - loss: 0.3916 - acc: 0.8253 - val_loss: 0.4431 - val_acc: 0.8108\n",
            "Epoch 6/10\n",
            "22500/22500 [==============================] - 146s 7ms/step - loss: 0.3648 - acc: 0.8414 - val_loss: 0.3551 - val_acc: 0.8428\n",
            "Epoch 7/10\n",
            "22500/22500 [==============================] - 146s 6ms/step - loss: 0.3398 - acc: 0.8530 - val_loss: 0.3426 - val_acc: 0.8472\n",
            "Epoch 8/10\n",
            "22500/22500 [==============================] - 147s 7ms/step - loss: 0.3193 - acc: 0.8622 - val_loss: 0.3373 - val_acc: 0.8500\n",
            "Epoch 9/10\n",
            "22500/22500 [==============================] - 147s 7ms/step - loss: 0.3029 - acc: 0.8722 - val_loss: 0.3260 - val_acc: 0.8540\n",
            "Epoch 10/10\n",
            "22500/22500 [==============================] - 147s 7ms/step - loss: 0.2875 - acc: 0.8807 - val_loss: 0.3138 - val_acc: 0.8624\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN-Bt0BVZUTn",
        "colab_type": "code",
        "outputId": "8ac0a984-ceff-4573-9cbe-10bc01a8a545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#model_bilstm_pretrain.load_weights('bilstm_pretrain_model.h5')\n",
        "model_bilstm_pretrain.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 59s 2ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.30916453993558884, 0.8671200275421143]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl-AU3fXpJue",
        "colab_type": "code",
        "outputId": "b44e4777-71df-49c2-92d1-0165ad04240b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "model_bilstm_pretrain_tuned = Sequential()\n",
        "model_bilstm_pretrain_tuned.add(Embedding(input_dim = embedding_matrix.shape[0], output_dim = embedding_matrix.shape[1], input_length = 300, weights = [embedding_matrix], trainable=False))\n",
        "model_bilstm_pretrain_tuned.add(Bidirectional(LSTM(50, recurrent_dropout=0.1)))\n",
        "model_bilstm_pretrain_tuned.add(Dropout(0.25))\n",
        "model_bilstm_pretrain_tuned.add(Dense(64))\n",
        "model_bilstm_pretrain_tuned.add(Dropout(0.5))\n",
        "model_bilstm_pretrain_tuned.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model_bilstm_pretrain_tuned.summary()\n",
        "\n",
        "model_bilstm_pretrain_tuned.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "history_bilstm_pretrain_tuned = model_bilstm_pretrain_tuned.fit(x_train, y_train, epochs=10, batch_size=128, validation_data=(x_val, y_val))\n",
        "model_bilstm_pretrain_tuned.save_weights('bilstm_pretrain_tuned_model.h5')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_15 (Embedding)     (None, 300, 100)          1000000   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 100)               60400     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 64)                6464      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 1,066,929\n",
            "Trainable params: 66,929\n",
            "Non-trainable params: 1,000,000\n",
            "_________________________________________________________________\n",
            "Train on 22500 samples, validate on 2500 samples\n",
            "Epoch 1/10\n",
            "22500/22500 [==============================] - 161s 7ms/step - loss: 0.6314 - acc: 0.6418 - val_loss: 0.5380 - val_acc: 0.7244\n",
            "Epoch 2/10\n",
            "22500/22500 [==============================] - 162s 7ms/step - loss: 0.5388 - acc: 0.7402 - val_loss: 0.5832 - val_acc: 0.6892\n",
            "Epoch 3/10\n",
            "22500/22500 [==============================] - 161s 7ms/step - loss: 0.4771 - acc: 0.7840 - val_loss: 0.4255 - val_acc: 0.8092\n",
            "Epoch 4/10\n",
            "22500/22500 [==============================] - 160s 7ms/step - loss: 0.4331 - acc: 0.8077 - val_loss: 0.4097 - val_acc: 0.8156\n",
            "Epoch 5/10\n",
            "22500/22500 [==============================] - 161s 7ms/step - loss: 0.4053 - acc: 0.8239 - val_loss: 0.3984 - val_acc: 0.8200\n",
            "Epoch 6/10\n",
            "22500/22500 [==============================] - 160s 7ms/step - loss: 0.3790 - acc: 0.8329 - val_loss: 0.3553 - val_acc: 0.8468\n",
            "Epoch 7/10\n",
            "22500/22500 [==============================] - 160s 7ms/step - loss: 0.3607 - acc: 0.8460 - val_loss: 0.3554 - val_acc: 0.8428\n",
            "Epoch 8/10\n",
            "22500/22500 [==============================] - 161s 7ms/step - loss: 0.3371 - acc: 0.8551 - val_loss: 0.3727 - val_acc: 0.8392\n",
            "Epoch 9/10\n",
            "22500/22500 [==============================] - 161s 7ms/step - loss: 0.3258 - acc: 0.8639 - val_loss: 0.3297 - val_acc: 0.8672\n",
            "Epoch 10/10\n",
            "22500/22500 [==============================] - 160s 7ms/step - loss: 0.3127 - acc: 0.8706 - val_loss: 0.3216 - val_acc: 0.8644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCsgS3gGZb1z",
        "colab_type": "code",
        "outputId": "6ca3ff41-5d16-4580-fa0e-1186fb8e1ce8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#model_bilstm_pretrain_tuned.load_weights('bilstm_pretrain_tuned_model.h5')\n",
        "model_bilstm_pretrain_tuned.evaluate(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 64s 3ms/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.31574368715286255, 0.8610000014305115]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRq4hQPupJnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbgffSlWpJlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}