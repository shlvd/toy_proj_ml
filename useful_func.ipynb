{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "useful_func.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0MV9V7xqj4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import scipy \n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score, max_error\n",
        "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.dummy import DummyRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import statsmodels.api as sm\n",
        "from io import StringIO\n",
        "\n",
        "def intitial_eda_checks(df):\n",
        "    if len(df[df.duplicated(keep=False)]) > 0:\n",
        "        print(df[df.duplicated(keep=False)])\n",
        "        df.drop_duplicates(keep='first', inplace=True)\n",
        "        print('Warning! df has been mutated!')\n",
        "    else:\n",
        "        print('No duplicates found.')\n",
        "\n",
        "    if df.isnull().sum().sum() > 0:\n",
        "        mask_total = df.isnull().sum().sort_values(ascending=False) \n",
        "        total = mask_total[mask_total > 0]\n",
        "\n",
        "        mask_percent = df.isnull().mean().sort_values(ascending=False) \n",
        "        percent = mask_percent[mask_percent > 0] \n",
        "\n",
        "        missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
        "    \n",
        "        print(f'Total and Percentage of NaN:\\n {missing_data}')\n",
        "    else: \n",
        "        print('No NaN found.')\n",
        "\n",
        "def view_columns_w_many_nans(df, missing_percent=.9):\n",
        "    mask_percent = df.isnull().mean()\n",
        "    series = mask_percent[mask_percent > missing_percent]\n",
        "    columns = series.index.to_list()\n",
        "    print(columns) \n",
        "    return columns\n",
        "\n",
        "def drop_columns_w_many_nans(df, missing_percent=.9):\n",
        "    series = view_columns_w_many_nans(df, missing_percent=missing_percent)\n",
        "    list_of_cols = series.index.to_list()\n",
        "    df.drop(columns=list_of_cols)\n",
        "    print(list_of_cols)\n",
        "    return df\n",
        "\n",
        "\n",
        "def histograms_numeric_columns(df, numerical_columns):\n",
        "    f = pd.melt(df, value_vars=numerical_columns) \n",
        "    g = sns.FacetGrid(f, col='variable',  col_wrap=4, sharex=False, sharey=False)\n",
        "    g = g.map(sns.distplot, 'value')\n",
        "    return g\n",
        "\n",
        "\n",
        "def boxplots_categorical_columns(df, categorical_columns, dependant_variable):\n",
        "    def boxplot(x, y, **kwargs):\n",
        "        sns.boxplot(x=x, y=y)\n",
        "        x=plt.xticks(rotation=90)\n",
        "\n",
        "    f = pd.melt(df, id_vars=[dependant_variable], value_vars=categorical_columns)\n",
        "    g = sns.FacetGrid(f, col='variable',  col_wrap=2, sharex=False, sharey=False, height=10)\n",
        "    g = g.map(boxplot, 'value', dependant_variable)\n",
        "    return g\n",
        "\n",
        "\n",
        "def heatmap_numeric_w_dependent_variable(df, dependent_variable):\n",
        "    plt.figure(figsize=(8, 10))\n",
        "    g = sns.heatmap(df.corr()[[dependent_variable]].sort_values(by=dependent_variable), \n",
        "                    annot=True, \n",
        "                    cmap='coolwarm', \n",
        "                    vmin=-1,\n",
        "                    vmax=1) \n",
        "    return g\n",
        "\n",
        "\n",
        "\n",
        "def high_corr_w_dependent_variable(df, dependent_variable, corr_value):\n",
        "    temp_df = df.corr()[[dependent_variable]].sort_values(by=dependent_variable, ascending=False)\n",
        "    mask_1 = abs(temp_df[dependent_variable]) > corr_value\n",
        "    return temp_df.loc[mask_1]\n",
        "\n",
        "\n",
        "\n",
        "def high_corr_among_independent_variable(df, dependent_variable, corr_value):\n",
        "    df_corr = df.drop(columns=[dependent_variable]).corr()\n",
        "    corr_dict = df_corr.to_dict()\n",
        "    temp_dict = {key_1: {key_2 : value \n",
        "                         for key_2, value in imbeded_dictionary.items() \n",
        "                         if abs(value) < 1 and abs(value) > corr_value}\n",
        "                for key_1, imbeded_dictionary in corr_dict.items()}\n",
        "    return {k:v for k, v in temp_dict.items() if v}\n",
        "\n",
        "\n",
        "\n",
        "def categorical_to_ordinal_transformer(categories):\n",
        "    return lambda categorical_value: categories.index(categorical_value)\n",
        "\n",
        "\n",
        "\n",
        "def transform_categorical_to_numercial(df, categorical_numerical_mapping):\n",
        "    transformers = {k: categorical_to_ordinal_transformer(v) \n",
        "                    for k, v in categorical_numerical_mapping.items()}\n",
        "    new_df = df.copy()\n",
        "    for col, transformer in transformers.items():\n",
        "        new_df[col] = new_df[col].map(transformer).astype('int64')\n",
        "    return new_df\n",
        "\n",
        "\n",
        "def dummify_categorical_columns(df):\n",
        "    categorical_columns = df.select_dtypes(include=\"object\").columns\n",
        "    return pd.get_dummies(df, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "\n",
        "\n",
        "def conform_columns(df_reference, df):\n",
        "    to_drop = [c for c in df.columns if c not in df_reference.columns]\n",
        "    return df.drop(to_drop, axis=1)\n",
        "\n",
        "\n",
        "def vizResids(model_title, X, y, random_state_number=42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=random_state_number)\n",
        "\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(X_train, y_train)\n",
        "\n",
        "    preds = lr.predict(X_test)\n",
        "    resids = y_test - preds\n",
        "    target_name = y.name.capitalize()\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12,10)) # 2 row x 2 columns\n",
        "    fig.suptitle(f\"{model_title}: $R^2$ test ={lr.score(X_test, y_test):2.2%}\", fontsize = 24, y = 1.05)\n",
        "\n",
        "    ax_1 = axes[0][0]\n",
        "    ax_2 = axes[0][1]\n",
        "    ax_3 = axes[1][0]\n",
        "\n",
        "    subplot_title_size = 18\n",
        "    subplot_label_size = 14\n",
        "    \n",
        "    ax_1.set_title(\"True Values ($y$) vs. Predictions ($\\hat{y}$)\", fontsize = subplot_title_size, pad = 10)\n",
        "    maxDist = max(max(preds),max(y)) \n",
        "    minDist = min(min(preds),min(y)) \n",
        "    ax_1.plot((minDist,maxDist),(minDist,maxDist), c = \"r\", alpha = .7);\n",
        "    \n",
        "    sns.scatterplot(ax = ax_1, x = y_test, y = preds, alpha = .5)\n",
        "    ax_1.set_xlabel(\"True Values ($y$)\", fontsize = subplot_label_size, labelpad = 10)\n",
        "    ax_1.set_ylabel(\"Predictions ($\\hat{y}$)\", fontsize = subplot_label_size, labelpad = 10)\n",
        "\n",
        "    ax_2.set_title(\"Residuals\", fontsize = subplot_title_size)\n",
        "    sns.scatterplot(ax = ax_2, x = range(len(resids)),y = resids, alpha = .5)\n",
        "    ax_2.set_ylabel(target_name, fontsize = subplot_label_size)\n",
        "    ax_2.axhline(0, c = \"r\", alpha = .7);\n",
        "\n",
        "    ax_3.set_title(\"Histogram of residuals\", fontsize = subplot_title_size)\n",
        "    sns.distplot(resids, ax = ax_3, kde = False);\n",
        "    ax_3.set_xlabel(target_name, fontsize = subplot_label_size)\n",
        "    ax_3.set_ylabel(\"Frequency\", fontsize = subplot_label_size)\n",
        "\n",
        "    plt.tight_layout() \n",
        "\n",
        "\n",
        "def error_metrics(y_true, y_preds, n, k):\n",
        "    def r2_adj(y_true, y_preds, n, k):\n",
        "        rss = np.sum((y_true - y_preds)**2)\n",
        "        null_model = np.sum((y_true - np.mean(y_true))**2)\n",
        "        r2 = 1 - rss/null_model\n",
        "        r2_adj = 1 - ((1-r2)*(n-1))/(n-k-1)\n",
        "        return r2_adj\n",
        "    \n",
        "    print('Mean Square Error: ', mean_squared_error(y_true, y_preds))\n",
        "    print('Root Mean Square Error: ', np.sqrt(mean_squared_error(y_true, y_preds)))\n",
        "    print('Mean absolute error: ', mean_absolute_error(y_true, y_preds))\n",
        "    print('Median absolute error: ', median_absolute_error(y_true, y_preds))\n",
        "    print('R^2 score:', r2_score(y_true, y_preds))\n",
        "    print('Adjusted R^2 score:', r2_adj(y_true, y_preds, n, k))\n",
        "\n",
        "\n",
        "def extract_individual_summary_table_statsmodel(X, y, table_number):\n",
        "    X = sm.add_constant(X)\n",
        "    y = y\n",
        "    model = sm.OLS(y,X).fit()\n",
        "    summary_df = StringIO(model.summary().tables[table_number].as_csv())\n",
        "    meta_df = pd.read_csv(summary_df)\n",
        "    return meta_df\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}