{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch_dlcourse_ai.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV52XfyPEv6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
        "\n",
        "from torchvision import transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDk0pCWfMoWZ",
        "colab_type": "code",
        "outputId": "03f50de1-fddb-4ba6-9b26-f0a881647cec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        }
      },
      "source": [
        "# First, lets load the dataset\n",
        "transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.43,0.44,0.47],std=[0.20,0.20,0.20])])                          \n",
        "data_train = dset.SVHN('./data/', split='train',transform=transform,download=True)\n",
        "data_test = dset.SVHN('./data/', split='test', transform=transform,download=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./data/train_32x32.mat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 181141504/182040794 [00:12<00:00, 22756023.20it/s]\n",
            "0it [00:00, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/test_32x32.mat to ./data/test_32x32.mat\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/64275384 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 16384/64275384 [00:00<09:01, 118676.24it/s]\u001b[A\n",
            "  0%|          | 40960/64275384 [00:00<08:09, 131356.42it/s]\u001b[A\n",
            "  0%|          | 57344/64275384 [00:00<08:22, 127672.45it/s]\u001b[A\n",
            "  0%|          | 98304/64275384 [00:00<06:57, 153880.61it/s]\u001b[A\n",
            "  0%|          | 139264/64275384 [00:00<05:56, 179760.61it/s]\u001b[A\n",
            "  0%|          | 204800/64275384 [00:01<04:50, 220854.15it/s]\u001b[A\n",
            "  0%|          | 270336/64275384 [00:01<04:03, 262838.82it/s]\u001b[A\n",
            "  1%|          | 344064/64275384 [00:01<03:26, 309882.70it/s]\u001b[A\n",
            "  1%|          | 434176/64275384 [00:01<02:53, 367635.99it/s]\u001b[A\n",
            "  1%|          | 532480/64275384 [00:01<02:28, 429769.57it/s]\u001b[A\n",
            "  1%|          | 647168/64275384 [00:01<02:06, 502185.70it/s]\u001b[A\n",
            "  1%|          | 778240/64275384 [00:01<01:48, 584422.16it/s]\u001b[A\n",
            "  1%|▏         | 925696/64275384 [00:02<01:33, 675752.90it/s]\u001b[A\n",
            "  2%|▏         | 1114112/64275384 [00:02<01:19, 795873.84it/s]\u001b[A\n",
            "  2%|▏         | 1327104/64275384 [00:02<01:07, 930437.52it/s]\u001b[A\n",
            "  2%|▏         | 1605632/64275384 [00:02<00:56, 1109103.09it/s]\u001b[A\n",
            "  3%|▎         | 1941504/64275384 [00:02<00:47, 1324653.79it/s]\u001b[A\n",
            "  4%|▎         | 2334720/64275384 [00:02<00:39, 1577275.96it/s]\u001b[A\n",
            "  4%|▍         | 2818048/64275384 [00:02<00:32, 1886531.22it/s]\u001b[A\n",
            "  5%|▌         | 3407872/64275384 [00:03<00:26, 2264939.18it/s]\u001b[A\n",
            "  6%|▋         | 4104192/64275384 [00:03<00:22, 2700472.13it/s]\u001b[A\n",
            "  8%|▊         | 4898816/64275384 [00:03<00:18, 3223802.68it/s]\u001b[A\n",
            "  9%|▉         | 5726208/64275384 [00:03<00:15, 3739601.91it/s]\u001b[A\n",
            " 10%|█         | 6733824/64275384 [00:03<00:13, 4376960.33it/s]\u001b[A\n",
            " 12%|█▏        | 7872512/64275384 [00:03<00:11, 5089901.78it/s]\u001b[A\n",
            " 14%|█▍        | 9158656/64275384 [00:03<00:09, 5886794.84it/s]\u001b[A\n",
            " 17%|█▋        | 10633216/64275384 [00:04<00:07, 6799287.79it/s]\u001b[A\n",
            " 19%|█▉        | 12361728/64275384 [00:04<00:06, 7874380.98it/s]\u001b[A\n",
            " 22%|██▏       | 14376960/64275384 [00:04<00:05, 9037810.36it/s]\u001b[A\n",
            " 26%|██▌       | 16695296/64275384 [00:04<00:04, 10594545.17it/s]\u001b[A\n",
            " 30%|██▉       | 19021824/64275384 [00:04<00:03, 11915924.47it/s]\u001b[A\n",
            " 34%|███▎      | 21676032/64275384 [00:04<00:03, 13441516.99it/s]\u001b[A\n",
            " 38%|███▊      | 24600576/64275384 [00:04<00:02, 15086588.49it/s]\u001b[A\n",
            " 43%|████▎     | 27697152/64275384 [00:04<00:02, 16717659.51it/s]\u001b[A\n",
            " 48%|████▊     | 30842880/64275384 [00:05<00:01, 18153288.23it/s]\u001b[A\n",
            " 52%|█████▏    | 33628160/64275384 [00:05<00:01, 20270703.12it/s]\u001b[A\n",
            " 56%|█████▌    | 35864576/64275384 [00:05<00:01, 19394699.37it/s]\u001b[A\n",
            " 59%|█████▉    | 37961728/64275384 [00:05<00:01, 18169678.68it/s]\u001b[A\n",
            " 64%|██████▎   | 40845312/64275384 [00:05<00:01, 19927639.31it/s]\u001b[A\n",
            " 67%|██████▋   | 43384832/64275384 [00:05<00:00, 21128638.88it/s]\u001b[A\n",
            " 71%|███████   | 45613056/64275384 [00:05<00:00, 20448649.91it/s]\u001b[A\n",
            " 74%|███████▍  | 47751168/64275384 [00:05<00:00, 19329112.21it/s]\u001b[A\n",
            " 78%|███████▊  | 50282496/64275384 [00:06<00:00, 19938292.15it/s]\u001b[A\n",
            " 83%|████████▎ | 53346304/64275384 [00:06<00:00, 22053505.73it/s]\u001b[A\n",
            " 87%|████████▋ | 55664640/64275384 [00:06<00:00, 22372808.20it/s]\u001b[A\n",
            " 90%|█████████ | 57982976/64275384 [00:06<00:00, 20500079.56it/s]\u001b[A\n",
            " 94%|█████████▎| 60121088/64275384 [00:06<00:00, 20624538.32it/s]\u001b[A\n",
            " 98%|█████████▊| 62734336/64275384 [00:06<00:00, 20340711.82it/s]\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfa4OHg2Mob6",
        "colab_type": "code",
        "outputId": "c1b5ca34-3eb8-4f8e-db49-70702ca10cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "data_train[0][0][0].shape\n",
        "plt.imshow(data_train[0][0][0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb4a6740748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwqCU1CVJ4Tm",
        "colab_type": "code",
        "outputId": "a28545c7-9dfc-407d-98b6-b921d946ea2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data_train.data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(73257, 3, 32, 32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vroQy5sqMogh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "data_size = data_train.data.shape[0]\n",
        "validation_split = .2\n",
        "split = int(np.floor(validation_split * data_size))\n",
        "indices = list(range(data_size))\n",
        "np.random.shuffle(indices)\n",
        "\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, \n",
        "                                           sampler=train_sampler)\n",
        "val_loader = torch.utils.data.DataLoader(data_train, batch_size=batch_size,\n",
        "                                         sampler=val_sampler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUn5fONHMojW",
        "colab_type": "code",
        "outputId": "d4a64e8c-dddb-4575-d0b7-1dde13cdab7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample, label = data_train[0]\n",
        "print(\"SVHN data sample shape: \", sample.shape)\n",
        "# As you can see, the data is shaped like an image\n",
        "\n",
        "# We'll use a special helper module to shape it into a tensor\n",
        "class Flattener(nn.Module):\n",
        "    def forward(self, x):\n",
        "        batch_size, *_ = x.shape\n",
        "        return x.view(batch_size, -1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVHN data sample shape:  torch.Size([3, 32, 32])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL1qhEiaNavm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_model = nn.Sequential(\n",
        "            Flattener(),\n",
        "            nn.Linear(3*32*32, 100),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(100, 10), \n",
        "         )\n",
        "nn_model.type(torch.FloatTensor)\n",
        "\n",
        "# We will minimize cross-entropy between the ground truth and\n",
        "# network predictions using an SGD optimizer\n",
        "loss = nn.CrossEntropyLoss().type(torch.FloatTensor)\n",
        "optimizer = optim.SGD(nn_model.parameters(), lr=1e-2, weight_decay=1e-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQw5NDAhMomA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is how to implement the same main train loop in PyTorch. Pretty easy, right?\n",
        "\n",
        "def train_model(model, train_loader, val_loader, loss, optimizer, num_epochs, scheduler=None):    \n",
        "    loss_history = []\n",
        "    train_history = []\n",
        "    val_history = []\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train() # Enter train mode\n",
        "        \n",
        "        \n",
        "        loss_accum = 0\n",
        "        correct_samples = 0\n",
        "        total_samples = 0\n",
        "        for i_step, (x, y) in enumerate(train_loader):\n",
        "            prediction = model(x)    \n",
        "            loss_value = loss(prediction, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss_value.backward()\n",
        "            optimizer.step()\n",
        "            _, indices = torch.max(prediction, 1)\n",
        "            correct_samples += torch.sum(indices == y)\n",
        "            total_samples += y.shape[0]\n",
        "            \n",
        "            loss_accum += loss_value\n",
        "\n",
        "        ave_loss = loss_accum / (i_step + 1)\n",
        "        train_accuracy = float(correct_samples) / total_samples\n",
        "        val_accuracy = compute_accuracy(model, val_loader)\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "        loss_history.append(float(ave_loss))\n",
        "        train_history.append(train_accuracy)\n",
        "        val_history.append(val_accuracy)\n",
        "        \n",
        "        print(\"Average loss: %f, Train accuracy: %f, Val accuracy: %f\" % (ave_loss, train_accuracy, val_accuracy))\n",
        "        \n",
        "    return loss_history, train_history, val_history\n",
        "        \n",
        "def compute_accuracy(model, loader):\n",
        "    \"\"\"\n",
        "    Computes accuracy on the dataset wrapped in a loader\n",
        "    \n",
        "    Returns: accuracy as a float value between 0 and 1\n",
        "    \"\"\"\n",
        "    model.eval() # Evaluation mode\n",
        "    # TODO: Implement the inference of the model on all of the batches from loader,\n",
        "    #       and compute the overall accuracy.\n",
        "    # Hint: PyTorch has the argmax function!\n",
        "    correct_samples = 0\n",
        "    total_samples = 0\n",
        "    for i, (x, y) in enumerate(loader):\n",
        "        prediction = model(x)\n",
        "        _, indices = torch.max(prediction, 1)\n",
        "        correct_samples += torch.sum(indices == y)\n",
        "        total_samples += y.shape[0]\n",
        "\n",
        "    return float(correct_samples) / total_samples\n",
        "\n",
        "#loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZraldaA1Morf",
        "colab_type": "code",
        "outputId": "dcdba66b-6fc3-4889-890f-6f854e071d44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Since it's so easy to add layers, let's add some!\n",
        "\n",
        "# TODO: Implement a model with 2 hidden layers of the size 100\n",
        "nn_model = nn.Sequential(\n",
        "            Flattener(),\n",
        "            nn.Linear(3*32*32, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 10),\n",
        "       )\n",
        "nn_model.type(torch.FloatTensor)\n",
        "\n",
        "optimizer = optim.SGD(nn_model.parameters(), lr=1e-2, weight_decay=1e-1)\n",
        "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "182042624it [00:30, 22756023.20it/s]                               \n",
            "64282624it [00:25, 20340711.82it/s]                              \u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average loss: 2.184056, Train accuracy: 0.198802, Val accuracy: 0.236366\n",
            "Average loss: 2.006230, Train accuracy: 0.270672, Val accuracy: 0.327964\n",
            "Average loss: 1.801126, Train accuracy: 0.378920, Val accuracy: 0.401338\n",
            "Average loss: 1.704909, Train accuracy: 0.420640, Val accuracy: 0.429732\n",
            "Average loss: 1.677852, Train accuracy: 0.435075, Val accuracy: 0.432530\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIPiBP93Mowx",
        "colab_type": "code",
        "outputId": "7ce74b7e-3cb8-4010-81f0-d5bc168ff8b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# We heard batch normalization is powerful, let's use it!\n",
        "# TODO: Add batch normalization after each of the hidden layers of the network, before or after non-linearity\n",
        "# Hint: check out torch.nn.BatchNorm1d\n",
        "\n",
        "nn_model = nn.Sequential(\n",
        "            Flattener(),\n",
        "            nn.Linear(3*32*32, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.Linear(100, 10),\n",
        "         )\n",
        "\n",
        "optimizer = optim.SGD(nn_model.parameters(), lr=1e-3, weight_decay=1e-1)\n",
        "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss: 1.920230, Train accuracy: 0.379023, Val accuracy: 0.549928\n",
            "Average loss: 1.476825, Train accuracy: 0.594478, Val accuracy: 0.641458\n",
            "Average loss: 1.306699, Train accuracy: 0.653704, Val accuracy: 0.673947\n",
            "Average loss: 1.214027, Train accuracy: 0.681893, Val accuracy: 0.711283\n",
            "Average loss: 1.156646, Train accuracy: 0.698632, Val accuracy: 0.719883\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ft4cvZHeMofB",
        "colab_type": "code",
        "outputId": "cdeab50b-8caa-4de3-f3d1-8da2329bcc88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Learning rate annealing\n",
        "# Reduce your learning rate 2x every 2 epochs\n",
        "# Hint: look up learning rate schedulers in PyTorch. You might need to extend train_model function a little bit too!\n",
        "\n",
        "nn_model = nn.Sequential(\n",
        "            Flattener(),\n",
        "            nn.Linear(3*32*32, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.Linear(100, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(100),\n",
        "            nn.Linear(100, 10),\n",
        "         )\n",
        "optimizer = optim.SGD(nn_model.parameters(), lr=1e-2, weight_decay=1e-1)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
        "loss_history, train_history, val_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, 5, scheduler)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average loss: 1.594373, Train accuracy: 0.518769, Val accuracy: 0.551225\n",
            "Average loss: 1.435015, Train accuracy: 0.607515, Val accuracy: 0.622210\n",
            "Average loss: 1.392833, Train accuracy: 0.652834, Val accuracy: 0.659887\n",
            "Average loss: 1.394356, Train accuracy: 0.653653, Val accuracy: 0.613405\n",
            "Average loss: 1.318091, Train accuracy: 0.698717, Val accuracy: 0.668077\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEV1t4yAMoY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlHpikgtMoT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL0t2i4zEvwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev20N50MEvu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "144Z7f-PEvsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}